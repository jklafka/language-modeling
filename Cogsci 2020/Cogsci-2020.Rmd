---
title: "Broad consistency but cross-lingusitic variation in the distribution of information in sentences"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{jklafka@andrew.cmu.edu} and \texttt{yurovsky@cmu.edu} \\
        Department of Psychology \\ Carnegie Mellon University}

abstract: >
    Optimal coding theories of language predict that speakers should keep the amount of information in their utterances relatively uniform under the constraints imposed by their language. But how much of a role do these constraints provide, and does it vary across languages? We find a consistent non-uniform shape which characterizes both spoken and written sentences of English but is tempered by predictive context. We then show that other languages are also characterized by consistent but non-English shaped curves related to their typological features, but that sufficient context produces more uniform shapes across languages. Thus, producers of language appear to structure their utterances in similar near-uniform ways despite varying linguistic contraints.
    
keywords: >
    information theory; efficient communication; language typology
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache = T, 
                      message=F, sanitize = T)

options(digits = 2)
```

```{r, libraries, include=FALSE}
require(png)
require(grid)
require(xtable)
require(here)
require(feather)
require(tidyverse)
require(tidytext)
library(tidyboot)
library(ggthemes)
library(directlabels)
library(widyr)
```

```{r preprocess-data}
lengths <- read_csv(here("Data/Paper/lengths.csv"))
# 
# b <- read_csv(here("Data/final_barycenters.csv")) 
# bnc <- read_csv(here("Surprisals/adult/unigram/bnc_compressed.csv"),
#                 col_names = F) %>%
#   mutate(id = 1:n()) %>%
#   pivot_longer(-id, names_to = "position", values_to = "surprisal") %>%
#   filter(complete.cases(.)) %>%
#   mutate(position = str_extract(position, "\\d+"),
#          position = as.numeric(position)) %>%
#   group_by(id) %>%
#   mutate(length = max(position)) %>%
#   ungroup() %>%
#   select(-id)
bnc <- read_csv(here("Data/bnc_lengths.csv"))

b <- read_csv(here("Data/5barycenters.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

# b <- read_csv(here("Data/7barycenters.csv"), 
#               col_names = c("1", "2", "3", "4", "5", "6", "7", 
#                             "language", "source", "gram"))

tidy_b <- b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


non_wiki_b <- read_csv(here("Data/5barycenters_new.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

tidy_non_wiki_b <- non_wiki_b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


# wals <- read_csv(here("Data/final_features.csv"))
wals <- read_csv(here("Data/Paper/wals_distances.csv"))
wals_types <- read_csv(here("Data/Paper/wals_type_distances.csv"))
swadesh <- read_csv(here("Data/Paper/swadesh_distances.csv"))
cosines <- read_csv(here("Data/Paper/cosines.csv")) 

bnc_surprisals <- read_csv(here("Data/Paper/bnc_subset.csv")) %>%
    mutate(gram = factor(gram, levels = c("unigram", "trigram")))

```

```{r theme}
theme_set(theme_few())
```

# Introduction

We use language for a variety of purposes like greeting friends, making records, and signaling group identity. These purposes all share a common goal: Transmitting information that changes the mental state of the listener [@austin1975]. For this reason, language can be thought of as a code, one that allows speakers to turn their intended meaning into a message that can be transmitted to a listener, and subsequently converted by the listener back into an approximation of the intended meaning [@shannon1948]. How should we expect this code to be structured?

If language has evolved to be a code for information transmission, its structure should reflect this process of optimization [@anderson1989]. The optimal code would have to work with two competing pressures: (1) For listeners to easily and successfully decode messages sent by the speaker, and (2) For speakers to easily code their messages and transmit them with minimal effort and error. A fundamental constraint on both of these processes is the linear order of spoken language--sounds are produced one at a time and each is unavailable perceptually once it is no longer being produced. 

Humans accomodate this linear order constraint through incremental processing. People process speech continuously as it arrives, predicting upcoming words and building expectations about the likely meaning of utterances in real-time rather than at their conclusion [@kutas2011; @tanenhaus1995]. Since prediction errors can lead to severe processing costs and difficulty integrating new information on the part of listeners, speakers should seek to minimize the chance of prediction errors when choosing what to say. However, the cost of producing more predictable utterances and thus minimizing the likelihood of errors is using more words. Therefore the optimal strategy for speakers seeking to minimize their production costs is to produce utterances that are just at the prediction capacity of listeners without exceeding this capacity [@aylett2004; @genzel2002]. In other words, speakers should consistently transmit information as close to the listener's fastest decoding rate as possible.

This Uniform Information Density hypothesis has found support at a variety of levels of language from the structure of individual words to the syntactic structure of utterances [see @gibson2019 for a review]. Further, speakers make online word choices that smooth out the information in their utterances [@jaeger2007]. While speakers can make bottom-up choices such as controlling which of several near-synonyms they produce or whether to produce an optional complementizer like "that," they cannot control the grammatical properties of their language. Properties like canonical word order impose top-down constraints on how speakers can structure what they say (e.g. English speakers obey a subject-verb-object word order constraint). While speakers may produce utterances as uniform in information density as their languages will allow, these top-down constraints may create significant and unique variation across languages. 

How significant are a language's top-down constraints on speakers? @yu2016 analyzed how the information in words of English sentences of a fixed length varies with their order in the sentence (e.g. first word, second word, etc). They found a surprising three-step shape where information first rises, then plateaus, and then sharply rises again at the ends of sentences. They argued that this non-linear shape may arise from top-down grammatical constraints on language. We build on these ideas, asking (1) Whether this shape depends on listener's predictive models, (2) Whether this shape varies across linguistic contexts, and (3) Whether this shape is broadly characteristic of a diverse set of languages or varies predictably from language to language. We find that languages are characterized by highly-reliable but cross-linguistically variable information structures that co-vary with top-down linguistic features. Listeners' predictive coding flattens these shapes across languages, in accord with predictions of the Uniform Information Density hypothesis.

# Study 1: Information in Written English

An influentual early test of the Uniform Information Density hypothesis was performed by @genzel2002, who analyzed the amount of information in successive sentences of the same text. They found that the amount of information increased across sentences when each was considered in isolation. They reasoned that since all prior sentences provide the context for reading each new sentences, the amount of total information (context + paragraph) was overall constant for human readers.

@yu2016 applied this same logic to analysis of the information in individual sentences, computing the entropy of each successive word in an utterance. Surprisingly, they found found a distinctive distribution for information in a corpus of written English. The first word of each sentence tended to contain little information; words in the middle of sentences each contained roughly the same amount of information as each other, but the final word of each sentence contained much more information than any other word. They found the same distribution across sentence lengths, from sentences with $15$ words to sentences with $45$ words. They took this as evidence against the Uniform Information Density Hypothesis as, unlike Genzel and Charniak's [-@genzel2002] results, information plateaud in the middle of sentences.

We replicate their analysis here, bringing it more in line with Genzel and Charniak's [-@genzel2002] methods. While Yu et al. [-@yu2016] considered only the information in each word, we build two different models. The first, replicating their analysis, considers the information of each word read in isolation. The second, following @genzel2002, is a trigram model that considers the surprisal of each word having read the prior two words. We take this trigram model as a better proxy for human reader's processing of these sentences. Finally, we also develop a method for averaging the curves for sentences of different lengths together to provide a single typical information structure.

## Data

Following @yu2016, we selected the British National Corpus (BNC) for analysis [@british-national-corpus-consortium2007]. The British National Corpus is ~100 million word corpus consisting of spoken (10%) and written (90%) English from the late 20th Century.

## Pre-processing

```{r bnc-setup}
MIN <- 5
MAX <- 45

bnc_total <- lengths %>%
    filter(language == "bnc") %>%
    pull(n) %>%
    sum()

bnc_kept <- lengths %>% 
    filter(length >= MIN, length <= MAX) %>% 
    summarise(n = sum(n)) %>%
    mutate(n = (n/bnc_total) * 100) %>%
    pull(n) %>%
    round(2)
    
```

We began with the XML version of the corpus, and used the \texttt{justTheWords.xsl} script provided along with the corpus to produce a text file with one sentence of the corpus on each line. Compound words (like "can't") were combined, and all words were converted to lowercase before analysis. This produced a corpus of just over six million utterance of varying lenghts. From these, we excluded utterances that were too short to allow for reasonable estimation of information shape (fewer than `r MIN` words), and utterances that were unusually long (more than `r MAX` words). This exclusion left us with `r bnc_kept`% of the utterances (Fig \ref{fig:bnc-lengths}.

```{r bnc-lengths, echo = F, fig.height = 2, fig.align = "center", fig.cap="The distribution of sentence lengths in the British National Corpus. We analyzed sentences of length 5-45 (colored)."}

# raw length distribution
lengths %>%
    filter(language == "bnc", length <= 100) %>%
    mutate(group = if_else(length < MIN | length > MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1/1.33) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray"))

```

## Estimating information

To estimate how information is distributed across utterances, we computed the lexical surprisal of each word under two different models [@levy2008; @shannon1948]. Intuitively, the surprisal of a word is a measure of how unexpected it would be to read that word, and thus how much information it contains. First, following @yu2016, we estimated a unigram model which considers each word independently: $\text{surprisal}(\text{word}) = -\log P(\text{word})$. This unigram surprisal measure is a direct transformation of the word's frequency and thus less frequent words are more surprising.

Second, we estimated a trigram model in which the surprisal of a given word ($w_i$) encodes how unexpected it is to read it after reading the prior two words ($w_{i-1}$ and $w_{i-2}$): $\text{surprisal}(w_{i}) = -log P(w_i|w_{i-1},w_{i-2})$. This metric encodes the idea that words that are low frequency in isolation (e.g. "meatballs") may become much less surprising in certain contexts (e.g. "spaghetti and meatballs") but more surprising in others (e.g. "coffee with meatballs"). In principle, we would like to encode the surprisal of a word given all of the prior sentential context However, the difficulty of correctly estimating these probabilities from a corpus grows combinatorically with the number of prior words, and in practice trigram models perform well as an approximation [see e.g. @chen1999; @smith2013].

### Model details

We estimated the surprisal for each word type in the British National Corpus using the KenLM toolkit [@heafield2013]. Each utterance was padded with a special start-of-sentence word "$\left<s\right>$" and end of sentence word "$\left</s\right>$". Trigram estimates did not cross sentence boundaries, so for example the surprisal of the second word in an utterances was estimated as ($\text{surprisal}(w_{2}) = -P(w_2|w_{i},\left<s\right>)$. 

Naïve trigram models will underestimate the surprisal of words in low-frequency trigrams (e.g. if the word "meatballs" appears only once in the corpus following exactly the words "spaghetti and", it is perfectly predictable from its prior two words). To avoid this underestimation, we used modified Kneser-Ney smoothing: this method discounts all ngram frequency counts and interpolates lower-order ngrams into the calcuations. These lower-order ngrams are weighted according to the number of distinct contexts they occur as a continuation [see @chen1999]. 

### Averaging curves

To develop a characteristic information curve for sentences in the corpus, we needed to aggregate sentences that varied dramatically in length (Fig \ref{bnc-plots}(A)). We used Dynamic Time Warping Barycenter Averaging (DBA), an algorithm for finding the average of sequences that share and underlying pattern but vary in length [@petitjean2011]. DBA inverts standard dynamic time warping, discovering a latent invariant template from a set of sequences. 

We used DBA to discover the short sequence of surprisal values that characterized the surprisal curves common to sentences of varying sentence lengths. We first averaged individual sentences of the same length together and then applied the DBA algorithm to this set of average sequences. DBA requires a parameter specifying the length of the template sequence. We chose a 5-length template sequence based on our inspection of the curves from the British National Corpus as well as the curves we found in subsequent studies. However, the results of this and the following studies were robust to other choices of this length parameter (7 and 10).

## Results and Discussion

```{r bnc_raw_full, eval = F}
bnc_unigram_surprisals <- read_csv(here("Surprisals/adult/unigram/bnc.csv"))
bnc_trigram_surprisals <- read_csv(here("Surprisals/adult/trigram/bnc.csv"))

bnc_subset_unigram <- bnc_unigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "unigram")

bnc_subset_trigram <- bnc_trigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "trigram")

write_csv(bind_rows(bnc_subset_unigram, bnc_subset_trigram),
          here("Data/Paper/bnc_subset.csv"))

```

```{r bnc_plots, fig.height = 4, fig.pos = "tb", fig.align = "center", fig.cap = "(A) Surprisal by sentence position of length 15, 30, and 45 sentences in the British National Corpus under unigram and trigram surprisal models. Error bars indicate 95\\% confidence intervals (tiny due to sample size). (B) Characteristic information curves produced by the DBA algorithm averaging over all sentence lengths in each corpus. "}
bnc_annotations <- tibble(position = c(12, 27, 42),
                          length = c(15, 30, 45),
                          gram = c("unigram", "unigram", "unigram"),
                          mean= c(4, 4, 4))

bnc_plot1 <- ggplot(bnc_surprisals, aes(x = position, y = mean, color = as.factor(length),
                           linetype = gram, label = length)) + 
    geom_errorbar(aes(ymin = mean - 1.96 * sem, ymax = mean + 1.96 * sem), width = 0) + 
    geom_line(position = position_dodge(.5)) + 
    scale_color_ptol() + 
    scale_linetype_manual(values = c("solid", "longdash")) + 
    geom_text(data = bnc_annotations) + 
    annotate("text", x = 6.5, y = 3.5, label = "Unigram") + 
    annotate("text", x = 6.5, y = 2.5, label = "Trigram") + 
    labs(x = "Sentence position", y = "Surprisal") + 
    labs(tag = "A") + 
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1/1.33)


bnc_b_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.5, 2))

bnc_plot2 <- tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), language %in% c("bnc")) %>% 
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = bnc_b_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("Coordinate") + 
    ylab("Surprisal") + 
    scale_color_colorblind() + 
    labs(tag = "B") + 
    theme_few() +
    theme(aspect.ratio = 1/1.33, legend.position = "none")

gridExtra::grid.arrange(bnc_plot1, bnc_plot2, ncol = 1)
```

We began by replicating Yu et al.'s [-@yu2016] analyses, examining the surprisal of words in sentence of length 15, 30, and 45 estimated by our unigram model. In line with their computations, we found a reliably non-linear shape in sentences of all 3 lengths, with the information in each word rising for the first two words, plateauing in the middle of sentences, dipping in pen-ultimate position, and rising steeply on the final word (Fig. \ref{fig:bnc_plots}A). 

In comparison, under the trigram model we observed 3 major changes. First, each word contained significantly less information. This is to be expected as knowing two prior words makes it easier to predict the next word. Second, the fall and peak at the ends of utterances was still observable, but much less pronounced. Finally, the first word of each sentence was now much more surpising than the rest of the words in the sentence (although still less surprising than the first word under the unigram model). This is because the model had only the start of sentence token $\left<s\right>$ to use as context. 

Together, these results suggest that @yu2016 overestimated the non-uniformity of information in sentences. Because readers of English beginning a new sentence have read the previous sentence, they can rely on this context to process the first word [while the model cannot; @genzel2002]. Thus, the trigram model likely overestimates the information for humans reading the first word. Second, the information in the final two words is closer to uniform with the rest of the sentence than under the trigram model. Nonetheless, the final words of utterances do consistently contain more information than the other words.

Finally, Fig. \ref{fig:bnc_plots}B shows the barycenter produced by DBA. The algorithm correctly recovers both the initial and final rise in information under the unigram model, and the initial fall and smaller final rise in the trigram model. We take this as evidence that (1) these shapes are characteristic not just of sentences of length 15, 30, and 45, but of all lengths, and (2) that DBA effectively recovers the characteristic structure of utterances of varying lengths.


```{r spoken_figs, fig.env = "figure*", fig.pos = "tb", fig.height = 4, fig.width = 7, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "(A) The distribution of sentence lengths in the spoken English corpora: Adults in Santa Barbara, and parents and children in CHILDES. We analyzed sentences of length 5-15 (colored). (B) Charateristic surprisal curves for these corpora."}
SPOKEN_MIN <- 5
SPOKEN_MAX <- 15

# raw length distribution
spoken_plot1 <- lengths %>%
    filter(language != "bnc", length <= 40) %>%
    mutate(corpus = case_when(language == "sbc" ~ "adults",
                              source == "adult" ~ "parents",
                              T ~ "children"),
           corpus = factor(corpus, levels = c("adults", "parents",
                                              "children")),
           group = if_else(length < SPOKEN_MIN | length > SPOKEN_MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    facet_wrap(~ corpus, scales = "free_y") +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1/1.33) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray")) + 
    labs(tag = "A")



spoken_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.25, 1.75),
                          plot_person = c("adults", "adults")) %>%
    mutate(plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children")))

spoken_plot2 <- tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), 
           language %in% c("sbc", "childes")) %>% 
    mutate(plot_person = case_when(language == "sbc" ~ "adults",
                                   source == "adult" ~ "parents",
                                   T ~ "children"),
           plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children"))) %>%
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    facet_wrap(~ plot_person) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = spoken_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("barycenter coordinate") + 
    ylab("surprisal") + 
    scale_color_colorblind() + 
    theme(legend.position = "none", aspect.ratio = 1/1.33) +
    labs(tag = "B")

gridExtra::grid.arrange(spoken_plot1, spoken_plot2, ncol = 1)
#cowplot::plot_grid(spoken_plot1, spoken_plot2, axis = 'b', nrow = 1)
```


In sum, the results of Study 1 suggest that sentences of written English have a characteristic non-uniform information structure, with information rising at the ends of sentences. This structure is more pronounced when each word is considered in isolation, but some of the structure remains even when each word is considered in context. Is this structure unique to written English, or does it characterize spoken English as well? In Study 2, we apply this same analysis to two corpora of spoken English--the first of adults speaking to other adults, and the second of adults and children speaking to each other.

# Study 2: Information in Spoken English


Spoken language is different from written language in several respects. First, the speed at which it can be processed is constrained by the speed at which it is produced. Second, speech occurs in a multimodal environment, providing listeners information from a variety of sources beyond the words conveyed (e.g. prosody, gesture, world context). Finally, the both words and sentence structures tend to be simpler in spoken language than written language as they must be produced and processed in real-time [@christiansen2016]. 

The language young children hear is further different from the language adults speak to each other. Child-directed speech tends to simpler than adult-directed speech on a number of dimensions including the lengths and prosodic contours of utterances, the diversity of words, and the complexity of syntactic structures [@snow1972]. The speech produced by young children is even more distinct from adult-adult speech, replete with simplifications and modifications imposed by their developing knowledge of both the lexicon and grammar [@clark2009].

In Study 2, we ask whether spoken English--produced both by adults and children-- has the same information structure as written English.

## Data

To estimate the information in utterances of adult-adult spoken English, we used the Santa Barbara Corpus of Spoken American English, $\sim$ 250,000 word corpus of recordings of naturally occurring spoken interactions from diverse regions of the United States [@sbc]. For parent-child interactions, we used all of the North American English corpora in the Child Language Data Exchange System (CHILDES) hosted hosted childes-db [@macwhinney2000; @sanchez2019]. We selected for analyses all $\sim$ 1 million utterances produced by children (mostly under the age of five), and $\sim$ 1.7 million utterances produced by the parents of these children.

## Data Processing

All pre-processing and modeling details were identical to Study 1 except for the selection of sentences for analysis. Because the utterances in both the Santa Barbara Corpus and CHILDES were significantly shorter than the sentences in the British National Corpus, we analyzed all utterances of at least 5 and most 15 words (see Fig. \ref{fig:spoken_figs}A). Models were estimated separately for each of the 3 corpora.

## Results and Discussion

The information curves found in adults-adult utterances were quite similar to those of parent-child utterances and child-parent utterances (Fig. \ref{fig:spoken_figs}B). Under the unigram model, information rose steeply in the beginnings of utterances, was relatively flatter in the middle of utterances, and the rose even more steeply at the ends. Under the trigram model, the first parts words of sentences contained the most information, information was relatively constant in the middle of utterances, and then rose slightly again at the ends. 

Unfortunately, we cannot compare amount of information across corpora--surprisal is highly correlated with corpus size (e.g. there is less information in adults' speech in Santa Babara than in children's speech in CHILDES). However, we can compare the shapes of these curves both to each-other and to the written English sentences in Study 1 \ref{fig:bnc_plots}B. All of these curves appeared to share their important qualitative features, including the sharp rise at the end under the unigram model and the attenuation of this rise under the trigram model. There are small differences--such as the flatter shape in the middle of written sentences than spoken utterances, but this difference is pronounced in the utterances of the Santa Barbara corpus relative to utterances of parents in CHILDES, suggesting that it may be partly a function of corpus size. 

Thus, English--both writen and spoken, both produced by adults and by children-appears to have a characteristic shape. Are the features of this shape features of English, or features of language more broadly? In Study 3 we apply this technique to a diverse set of written languages of different families to ask whether these structures vary cross-linguisticly. 

# Study 3: Cross-linguistic variation in information

## Data

```{r wiki-langs}
n_langs <- tidy_b %>% 
    filter(source == "wikipedia") %>% 
    distinct(language) %>% 
    count() %>%
    pull()

families <- read_csv(here("Data/Paper/families.csv"))

scaled_b <- tidy_b %>%
    group_by(language, gram) %>%
    mutate(surprisal = scale(surprisal, scale = FALSE))

families_b <- scaled_b %>%
    left_join(families, by = c("language")) 

n_fams <- families_b %>%
    ungroup() %>%
    distinct(family) %>%
    filter(!is.na(family), family != "other") %>%
    count() %>%
    pull()
```

To measures cross-linguistic variation in the structure of information across sentences, we constructed a corpus of Wikipedia articles from all languages with at least 10,000 articles. This resulted a set of `r n_langs` languages from `r n_fams` families. We then used two measures of lexical similarity to investigate cross-linguistic variation in information curves.

To target lexical differences between languages, we used the 40-item Swadesh word list consisting of basic concepts appearing in all langauges [@swadesh1955; @wichmann2016]. The more similar two languages' words are, the more similar the two languages are. To understand the relationship between information curves and typological features, we used the World Atlas of Language Structures, a collection of morphological, syntactic, phonological, and other features of linguistic structure [WALS; @2013]. As WALS is a compiled database from dozens of papers from different authors, most features for most languages are fairly sparse. We use a iterative imputation algorithm for categorical data Multiple Imputation Multiple Correspondence Analysis to fill in the missing features [MIMCA; @audigier2017]. 

## Data processing

All processing was identical to Studies 1 and 2 except for the lengths of utterances chosen for analyses. To accomodate the variety of lengths across language corpora, we analyzed sentences of lengths 5 to 30.

For each pair of languages, we derived three pairwise similarity measures. To estimate the information structure similarity, we first centered each language's 5-point curve (since surprisal is highly correlate with corpus size), and then computed the cosine similarity between the two centered curves. To estimate Swadesh similarity, we computed the average normalized Levenshtein distance between each of the 40 words. Finally, to compare typological similarity, we added the number of features two languages shared.

## Results and Discussion
```{r cor_data}
cor_data <- left_join(cosines, wals, by = c("language1", "language2")) %>%
    left_join(swadesh, by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy()),
           swadesh_cor = map(data, ~cor.test(.x$cosine, .x$ldn,
                                             use = "pairwise") %>% tidy()),
           dist_cor = map(data, ~cor.test(.x$wals_dist, .x$ldn, 
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    pivot_longer(-gram, names_to = "measure", values_to = "correlation") %>%
    unnest(cols = c(correlation))


type_cor_data <- left_join(cosines, wals_types, 
                           by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram, type) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    unnest(cols = c(wals_cor)) %>%
    ungroup() %>%
    mutate(type = factor(type, levels = c("nom_syntax", "clauses", 
                                          "verb_categories", "phonology", 
                                          "word_order", "nom_categories"),
                         labels = c("nominative syntax", "clauses", 
                                    "verb categories", "phonology", 
                                    "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

swadesh_stats_uni <- filter(cor_data, measure == "swadesh_cor", 
                            gram == "unigram")

swadesh_stats_tri <- filter(cor_data, measure == "swadesh_cor", 
                            gram == "trigram")

wals_stats_uni <- filter(cor_data, measure == "wals_cor", 
                            gram == "unigram")

wals_stats_tri <- filter(cor_data, measure == "wals_cor", 
                            gram == "trigram")

```

Unlike the striking consistency across multiple English corpora, we found significant variability in the structure of information curves across languages estimated under the unigram. Fig. \ref{fig:diff-languages} shows centered information curves for a sample of languages from several language families. Despite this variability, the shapes of information curves estimated under the trigram model were more similar cross-linguistically and to the shapes found in English: Sentences began with highly informative words (presumably due to lack of context) and then rapidly approached a uniform level of information. Consistent with this characterization, pairwise similarities in languages' information curves estimated under the unigram model were more correlated with their Swadesh distances ($r =$ `r swadesh_stats_uni$estimate`, $t =$ `r swadesh_stats_uni$statistic`, $p$ `r papaja::printp(swadesh_stats_uni$p.value)`) than their distances estimated under the trigram model ($r =$ `r swadesh_stats_tri$estimate`, $t =$ `r swadesh_stats_tri$statistic`, $p =$ `r papaja::printp(swadesh_stats_tri$p.value)`). 

```{r diff-languages, fig.height=3, fig.width = 3.5, fig.pos = "tb", fig.cap="Characteristic information curves (centered) for a sample of languages from Wikipedia"}
families_b %>%
    filter(language %in% c("Afrikaans", "Czech", 
                           "Japanese", "Tamil", "Kannada",
                           "Hebrew", "Somali", "Maltese")) %>%
    ggplot(aes(x = position, y = surprisal, color = language, label = language,
               group = interaction(language, family))) +
    facet_grid(gram ~ family, scales = "free_y") +
    geom_point() + 
    geom_line() +
    scale_color_ptol() + 
    geom_dl(method = list(dl.trans(x = x + 1.25), "first.points", cex = .6)) + 
    theme_few(base_size = 9) +
    labs(y = "normalized surprisal") +
    theme(legend.position = "none", aspect.ratio = 1.33)
```

While the trigram information curves have a more consistent qualitative shape, there are differences between languages. The pairwise similarities between languages' trigram information curves were more correlated with the number of WALS features they shared ($r =$ `r wals_stats_tri$estimate`, $t =$ `r wals_stats_tri$statistic`, $p$ `r papaja::printp(wals_stats_tri$p.value)`) than their similarities estimated under the trigram model ($r =$ `r wals_stats_uni$estimate`, $t =$ `r wals_stats_uni$statistic`,$p =$ `r papaja::printp(wals_stats_uni$p.value)`).

To understand which typological features contribute to these similarities, we split the WALS features by type: the nominative categories features describe aspects of morphology such as case systems and definite/indefinite articles; word order describes the relationship of the subject, verb, and object to each-other as well as head-modifier word order; nominative syntax describes noun behavior such as possessives and adjectives acting as nouns; clauses describes phrasal and broader sentence syntax; and verb categories describe tense, mood and aspect as well as morphology on the verb. Fig. \ref{fig:type_cors} shows the correlation between the similarity of information curves under both the unigram and trigram models and the number of features of each of these types two-languages shared. Under the unigram model, word order features appear to predict information curve similarity. In contrast, under the trigram model, all features types except for nominative syntax are reliably correlated with information curve similarity.

```{r type_cors, fig.height = 3.5, fig.cap="Pairwise correlations between languages' centered information curves and the number of linguistic features they share of each type. Error bars indicate 95\\% CIs."}
type_annotations <- tibble(type = c("phonology", "phonology"),
                          estimate = c(-.02, .1),
                          gram = c("unigram", "trigram")) %>%
    mutate(type = factor(type, levels = c("nominative syntax", "clauses", 
                                          "verb categories", "phonology", 
                                          "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

ggplot(type_cor_data, aes(x = type, y = estimate, color = gram)) + 
    geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
                    position = position_dodge(.5)) +
    geom_hline(aes(yintercept = 0), linetype = "dashed") + 
    scale_color_ptol() +
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1,
          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
    labs(x = "", y = "info curve and WALS corr.") + 
    geom_text(aes(label = gram), data = type_annotations)
```

Taken together, these results suggest three broad conclusions. First, aspects of the history of languages--encoded in their lexicostatistics--structure the shapes of information in typical sentences. When each word in a sentence is considered alone, languages vary quite dramatically in how information is distributed across sentences. Second, a diverse set of typological features of languages are related to how information is structured for listeners who bring predictive processing to language. These features appear to explain a small but reliable proportion of the variation in how uniformly information is distributed across utterances. Finally, despite this variation, two words of predictive context radically transform the structure of information in utterances, leading to significantly more uniformative in all languages irrespective of their typological structure. These analyses suggest that top-down constraints from language do play an important role in structuring speakers' utterances, but the speakers have tremendous power to choose efficient utterances within these constraints.

# Conclusion

In this paper we have proposed a novel method for quantifying how information is typically distributed across the words of a sentence. We showed that English--whether written or spoken, whether produced by adults or children--has a prototypical information structure. We then showed that this information structure varies cross-lingusitically in predictable ways, but also shares broad similarities. These results add to a growing body of research using information theory to explore the structure of language. While much of this work has considered the choices speakers make to structure their utterances within the constraints imposed by their linguistic codes, our work adds to this research program by investigating these constraints themselves. These results represent a small first step towards answering the question of how much these constraints shape speakers' productions, and how speakers interact with them.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
