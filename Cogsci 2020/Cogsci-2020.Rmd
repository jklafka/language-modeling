---
title: "Communication tends towards optimal decoding regardless of language"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{jklafka@andrew.cmu.edu} and \texttt{yurovsky@cmu.edu} \\
        Department of Psychology \\ Carnegie Mellon University}

abstract: >
    What role does our language play in how we structure our utterances, and how does this affect communication? In this paper, we present a novel method of examining how speakers communicate to their listeners in a given language, surveying numerous diverse languages. For each language, we quantify the information structure speakers of that language tend to use, regardless of age and medium (spoken or written). How speakers structure information depends in part on the top-down features in their language such as word order, but largely derives from bottom-up choices made by speakers during real-time communication. For listeners, the effects of these information structures disappear with predictive processing: using context, listeners process utterances according to a more constant rate predicted by optimal coding. Thus, although speakers must obey constraints from their language in communication, listeners at the other end of the communication channel can minimize errors in noisy channel processing. 
    
keywords: >
    information theory; efficient communication; language typology
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache = T, 
                      message=F, sanitize = T)
```

```{r, libraries, include=FALSE}
require(png)
require(grid)
require(xtable)
require(here)
require(feather)
require(tidyverse)
require(tidytext)
library(tidyboot)
library(ggthemes)
library(directlabels)
library(widyr)
```

```{r preprocess-data}
lengths <- read_csv(here("Data/Paper/lengths.csv"))
# 
# b <- read_csv(here("Data/final_barycenters.csv")) 
# bnc <- read_csv(here("Surprisals/adult/unigram/bnc_compressed.csv"),
#                 col_names = F) %>%
#   mutate(id = 1:n()) %>%
#   pivot_longer(-id, names_to = "position", values_to = "surprisal") %>%
#   filter(complete.cases(.)) %>%
#   mutate(position = str_extract(position, "\\d+"),
#          position = as.numeric(position)) %>%
#   group_by(id) %>%
#   mutate(length = max(position)) %>%
#   ungroup() %>%
#   select(-id)
bnc <- read_csv(here("Data/bnc_lengths.csv"))

b <- read_csv(here("Data/5barycenters.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

# b <- read_csv(here("Data/7barycenters.csv"), 
#               col_names = c("1", "2", "3", "4", "5", "6", "7", 
#                             "language", "source", "gram"))

tidy_b <- b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


non_wiki_b <- read_csv(here("Data/5barycenters_new.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

tidy_non_wiki_b <- non_wiki_b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


# wals <- read_csv(here("Data/final_features.csv"))
wals <- read_csv(here("Data/Paper/wals_distances.csv"))
wals_types <- read_csv(here("Data/Paper/wals_type_distances.csv"))
swadesh <- read_csv(here("Data/Paper/swadesh_distances.csv"))
cosines <- read_csv(here("Data/Paper/cosines.csv")) 

bnc_surprisals <- read_csv(here("Data/Paper/bnc_subset.csv")) %>%
    mutate(gram = factor(gram, levels = c("unigram", "trigram")))

```

```{r theme}
theme_set(theme_few())
```

# Introduction

We use language for a variety of purposes like greeting friends, making records, and signaling group identity. These purposes all share a common goal: Transmitting information that changes the mental state of the listener [@austin1975]. For this reason, language can be thought of as a code, one that allows speakers to turn their intended meaning into a message that can be transmitted to a listener, and subsequently converted by the listener back into an approximation of the intended meaning [@shannon1948]. How should we expect this code to be structured?

If language has evolved to be a code for information transmission, its structure should reflect this process of optimization [@anderson1989]. The optimal code would have to work with two competing pressures: (1) For listeners to easily and successfully decode messages sent by the speaker, and (2) For speakers to easily code their messages and transmit them with minimal effort and error. A fundamental constraint on both of these processes is the linear order of spoken language--sounds are produced one at a time and each is unavailable perceptually once it is no longer being produced. 

Humans accomodate this linear order constraint through incremental processing. People process speech continuously as it arrives, predicting upcoming words and building expectations about the likely meaning of utterances in real-time rather than at their conclusion [@kutas2011; @tanenhaus1995; @pickering2013]. Since prediction errors can lead to severe processing costs and difficulty integrating new information on the part of listeners, speakers should seek to minimize the chance of prediction errors when choosing what to say. However, the cost of producing more predictable utterances and thus minimizing the likelihood of errors is using more words. Therefore the optimal strategy for speakers seeking to minimize their production costs is to produce utterances that are just at the prediction capacity of listeners without exceeding this capacity [@aylett2004; @genzel2002]. In other words, speakers should consistently transmit information as close to the listener's fastest decoding rate as possible.

This Uniform Information Density hypothesis has found support at a variety of levels of language from the structure of individual words to the syntactic structure of utterances [@piantadosi2011; @jaeger2007; see @gibson2019 for a review]. Further, speakers make online word choices that smooth out the information in their utterances [@mahowald2013; @jaeger2007]. While speakers can make bottom-up choices such as controlling which of several near-synonyms they produce or whether to produce an optional complementizer like "that," they cannot control the grammatical properties of their language. Properties like canonical word order impose top-down constraints on how speakers can structure what they say (e.g. English speakers obey a subject-verb-object word order constraint). While speakers may produce utterances as uniform in information density as their languages will allow, these top-down constraints may create significant and unique variation across languages. 

How significant are a language's top-down constraints on speakers? @yu2016 analyzed how the information in words of English sentences of a fixed length varies with their order in the sentence (e.g. first word, second word, etc). They found a surprising three-step shape where information first rises, then plateaus, and then sharply rises again at the ends of sentences. They argued that this non-linear shape may arise from top-down grammatical constraints on language. We build on these ideas, asking (1) Whether this shape depends on listener's predictive models, (2) Whether this shape varies across linguistic contexts, and (3) Whether this shape is broadly characteristic of a diverse set of languages or varies predictably from language to language. We find that languages are characterized by highly-reliable but cross-linguistically variable information structures that co-vary with top-down linguistic features. Listeners' predictive coding flattens these shapes across languages, in accord with predictions of the Uniform Information Density hypothesis.

# Study 1: Information in Written English

The Uniform Information Density Hypothesis predicts that people should structure their utterances so that the amount of information in each unit of language remains nearly constant. An influentual early test of this idea was performed by @genzel2002, who analyzed the amount of information in successive sentences of the same text. They found that the amount of information increased across sentences when each was considered in isolation. They reasoned that since all prior sentences provide the context for reading each new sentences, the amount of total information (context + paragraph) was overall constant for human readers.

@yu2016 applied this same logic to analysis of the information in individual sentences, computing the entropy of each successive word in an utterance. Surprisingly, they found found a distinctive three-step distribution for information in a corpus of written English. The first word of each sentence tended to contain little information; words in the middle of sentences each contained roughly the same amount of information, but the final word of each sentence contained much more information than any other word. They found the same distribution across sentence lengths, from sentences with $15$ words to sentences with $45$ words. They took this as evidence against the Uniform Information Density Hypothesis as, unlike Genzel and Charniak's [-@genzel2002] results, information plateaud in the middle of sentences rather than increasing as it did at the beginnings and ends.

We replicate their analysis here, bringing it more in line with Genzel and Charniak's [-@genzel2002] methods. While Yu et al. [-@yu2016] considered only the information in each word, we build two different models. The first, replicating their analysis, considers the information of each word read in isolation. The second, following @genzel2002, is a trigram model that considers the surprisal of each word having read the prior two words. We take this trigram model as a better proxy for human reader's processing of these sentences. Finally, we also develop a method for averaging the curves for sentences of different lengths together to provide a single typical information curve signature.

## Corpus

Following @yu2016, we selected the British National Corpus (BNC) for analysis [@british-national-corpus-consortium2007]. The British National Corpus is ~100 million word corpus consisting of spoken (10%) and written (90%) English from the late 20th Century.

## Pre-processing

```{r bnc-setup}
MIN <- 5
MAX <- 45

bnc_total <- lengths %>%
    filter(language == "bnc") %>%
    pull(n) %>%
    sum()

bnc_kept <- lengths %>% 
    filter(length >= MIN, length <= MAX) %>% 
    summarise(n = sum(n)) %>%
    mutate(n = (n/bnc_total) * 100) %>%
    pull(n) %>%
    round(2)
    
```

We began with the XML version of the corpus, and used the \texttt{justTheWords.xsl} script provided along with the corpus to produce a text file with one sentence of the corpus on each line. Compound words (like "can't") were combined, and all words were converted to lowercase before analysis. This produced a corpus of just over six million utterance of varying lenghts. From these, we excluded utterances that were too short to allow for reasonable estimation of information shape (fewer than `r MIN` words), and utterances that were unusually long (more than `r MAX` words). This exclusion left us with `r bnc_kept`% of the utterances (Fig \ref{fig:bnc-lengths}.

```{r bnc-lengths, echo = F, fig.height = 2.5, fig.align = "center", fig.cap="The distribution of sentence lengths in the British National Corpus. We analyzed sentences of length 5-45 (colored)."}

# raw length distribution
lengths %>%
    filter(language == "bnc", length <= 100) %>%
    mutate(group = if_else(length < MIN | length > MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray"))

```

## Estimating information

To estimate how information is distributed across utterances, we computed the lexical surprisal of each word under two different models [@levy2008; @shannon1948]. Intuitively, the surprisal of a word is a measure of how unexpected it would be to read that word, and thus how much information it contains. First, following @yu2016, we estimated a unigram model which considers each word independently, asking how unexpected that word would be in the absence of any context: $\text{surprisal}(\text{word}) = -\log P(\text{word})$. This unigram surprisal measure is a direct transformation of the word's frequency and thus less frequent words are more surprising.

Second, we estimated a trigram model in which the surprisal of a given word ($w_i$) encodes how unexpected it is to read it after reading the prior two words ($w_{i-1}$ and $w_{i-2}$): $\text{surprisal}(w_{i}) = -log P(w_i|w_{i-1},w_{i-2})$. This metric encodes the idea that words that are low frequency in isolation (e.g. "meatballs") may become much less surprising in certain contexts (e.g. "spaghetti and meatballs") but more surprising in others (e.g. "coffee with meatballs"). In principle, we would like to encode the surprisal of a word given all of the prior sentential context ($\text{surprisal}(w_{i}) = -P(w_i|w_{i-1}w_{i-2}...w_{1})$. However, the difficulty of correctly estimating these probabilities from a corpus grows combinatorically with the number of prior words, and in practice trigram models perform well as an approximation [see e.g. @chen1999; @smith2013].

### Model details

We estimated the surprisal for each word type in the British National Corpus using the KenLM toolkit [@heafield2013]. Each utterance was padded with a special start-of-sentence word "$\left<s\right>$" and end of sentence word "$\left</s\right>$". Trigram estimates did not cross sentence boundaries, so for example the surprisal of the second word in an utterances was estimated as ($\text{surprisal}(w_{2}) = -P(w_2|w_{i},\left<s\right>)$. 

Na√Øve trigram models will underestimate the surprisal of words in low-frequency trigrams (e.g. if the word "meatballs" appears only once in the corpus following exactly the words "spaghetti and", it is perfectly predictable from its prior two words). To avoid this underestimation, we used modified Kneser-Ney smoothing: this method discounts all ngram frequency counts--reducing the impact of rare ngrams on probability calculations--and interpolates lower-order ngrams into the calcuations. These lower-order ngrams are weighted according to the number of distinct contexts they occur as a continuation [e.g. "Francisco" may be a common word in a corpus, but likely only occurs after "San" as in "San Francisco", so it receives a lower weighting; see @chen1999]. 

### Averaging curves

To develop a characteristic information curve for sentences in the corpus, we needed to aggregate sentences that varied dramatically in length (Fig \ref{bnc-plots}(A)). We used Dynamic Time Warping Barycenter Averaging (DBA), an algorithm for finding the average of sequences that share and underlying pattern but vary in length [@petitjean2011]. DBA is an extension of standard Dynamic Time Warping, which searches for an invariant template in shifted and stretched instances of that template [e.g. all instances of the vowel 'a' in acoustic recordings of a speaker's productions even if the instances vary in their duration and intensity]. DBA inverts this idea, discovering a latent invariant template from a set of sequences. 

We used DBA to discover the short sequence of surprisal values that characterized the surprisal curves common to sentences of varying sentence lengths. We first averaged individual sentences of the same length together, and then applied the DBA algorithm to this set of average sequences. DBA requires a parameter specifying the length of the template sequence. We chose 5 as the length of the template sequence based on our inspection of the curves from the British National Corpus as well as the curves we found in subsequent studies. However, the results of this and the following studies were robust to other choices of this length parameter (7 and 10).

## Results and Discussion

```{r bnc_raw_full, eval = F}
bnc_unigram_surprisals <- read_csv(here("Surprisals/adult/unigram/bnc.csv"))
bnc_trigram_surprisals <- read_csv(here("Surprisals/adult/trigram/bnc.csv"))

bnc_subset_unigram <- bnc_unigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "unigram")

bnc_subset_trigram <- bnc_trigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "trigram")

write_csv(bind_rows(bnc_subset_unigram, bnc_subset_trigram),
          here("Data/Paper/bnc_subset.csv"))

```

```{r bnc_plots, fig.height = 5.5, fig.width = 3, fig.pos = "tb", fig.align = "center", fig.cap = "(A) Surprisal by sentence position of length 15, 30, and 45 sentences in the British National Corpus under unigram and trigram surprisal models. Error bars indicate 95\\% confidence intervals (tiny due to sample size). (B) Characteristic information curves produced by the DBA algorithm averaging over all sentence lengths in each corpus. "}
bnc_annotations <- tibble(position = c(12, 27, 42),
                          length = c(15, 30, 45),
                          gram = c("unigram", "unigram", "unigram"),
                          mean= c(4, 4, 4))

bnc_plot1 <- ggplot(bnc_surprisals, aes(x = position, y = mean, color = as.factor(length),
                           linetype = gram, label = length)) + 
    geom_errorbar(aes(ymin = mean - 1.96 * sem, ymax = mean + 1.96 * sem), width = 0) + 
    geom_line(position = position_dodge(.5)) + 
    scale_color_ptol() + 
    scale_linetype_manual(values = c("solid", "longdash")) + 
    geom_text(data = bnc_annotations) + 
    annotate("text", x = 6.5, y = 3.5, label = "Unigram") + 
    annotate("text", x = 6.5, y = 2.5, label = "Trigram") + 
    labs(x = "Sentence position", y = "Surprisal") + 
    labs(tag = "A") + 
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1)


bnc_b_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.5, 2))

bnc_plot2 <- tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), language %in% c("bnc")) %>% 
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = bnc_b_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("Coordinate") + 
    ylab("Surprisal") + 
    scale_color_colorblind() + 
    labs(tag = "B") + 
    theme_few() +
    theme(aspect.ratio = 1, legend.position = "none")

gridExtra::grid.arrange(bnc_plot1, bnc_plot2, ncol = 1)
```

We began by replicating Yu et al.'s [-@yu2016] analyses, examining the surprisal of words in sentence of length 15, 30, and 45 estimated by our unigram model. In line with their computations, we found a reliably non-linear shape in sentences of all 3 lengths, with the information in each word rising for the first two words, plateauing in the middle of sentences, dipping in pen-ultimate position, and rising steeply on the final word (Fig. \ref{fig:bnc_plots}A). 

In comparison, under the trigram model we observed 3 major changes. First, each word contained significantly less information. This is to be expected as the knowing two prior words makes it much easier to predict the next word. Second, the fall and peak at the ends of utterances was still observable, but much less pronounced. Finally, the first word of each sentence was now much more surpising than the rest of the words in the sentence (although still less surprising than the first word under the unigram model). This is because the model had only the start of sentence token $\left<s\right>$ to use as context, but knowing that the word is the start of an utterance provides some constraints on the likley word. 

Together, these results suggest that @yu2016 overestimated the non-uniformity of information in sentences. Because readers of English beginning a new sentence have read the previous sentence, they can rely on this context to process the first word [while the model cannot; @genzel2002]. Thus, the trigram model likely overestimates the information for humans reading the first word, which is likely to be closer in information to words in the middle of the sentence. Second, the information in the final two words is closer to uniform with the rest of the sentence than under the trigram model. Nonetheless, the final words of utterances do consistently contain more information than the other words.

Finally, Fig. \ref{fig:bnc_plots}B shows the results produced by Dynamic Time Warping Barycenter Averaging (DBA) on all of the sentences of varying lengths in the Bristish National Corpus. The algorithm correctly recovers both the initial and final rise in information under the unigram model, and the initial fall and smaller final rise in the trigram model. We take this as evidence that (1) these shapes are characteristic not just of sentences of length 15, 30, and 45, but of all lengths, and (2) that DBA effectively recovers the characteristic structure of utterances of varying lengths.

In sum, the results of Study 1 suggest that sentences of written English have a characteristic non-uniform information structure, with information rising at the ends of sentences. This structure is more pronounced when each word is considered in isolation, but some of the structure remains even when each word is considered in context. Is this structure unique to written English, or does it characterize spoken English as well? In Study 2, we apply this same analysis to two corpora of spoken English--the first of adults speaking to other adults, and the second of adults and children speaking to each-other.

# Study 2: Information in Spoken English

Spoken language is different from written language in several respects. First, because spoken language isrequires a speaker to produce it, the speed at which it can be processed is constrained by the speed at which it is produced. Second, speech occurs in a multimodal environment, providing listeners information from a variety of sources beyond the words conveyed (e.g. prosody, gesture, world context). Finally, the both words and sentence structures tend to be simpler in spoken language than written language as they must be produced and processed in real-time [@christiansen2016]. Thus, sentences of spoken English may have different information curves than sentences of written English.

The language young children hear is further different from the language adults speak to each other. Child-directed speech tends to simpler than adult-directed speech on a number of dimensions including the lengths and prosodic contours of utterances, the diversity of words, and the complexity of syntactic structures [@fernald1989; @snow1972]. The speech produced by young children is even more different from adult-adult speech, replete with simplifications and modifications imposed by their developing knowledge of both the lexicon and grammar [@clark2009].

In Study 2, we ask whether spoken English--produced both by adults and children-- has the same information structure as written English.

## Corpus

To estimate the information in utterances of adult-adult spoken English, we used the Santa Barbara Corpus of Spoken American English, $\sim$ 250,000 word corpus of recordings of naturally occurring spoken interactions from diverse regions of the United States [@sbc]. For parent-child interactions, we used all of the North American English corpora in the Child Language Data Exchange System (CHILDES) hosted hosted childes-db [@macwhinney2000; @sanchez2019]. We selected for analyses all $\sim$ 1 million utterances produced by children (mostly under the age of five), and $\sim$ 1.7 million utterances produced by the parents of these children.

## Data Processing

All pre-processing and modeling details were identical to Study 1 except for the selection of sentences for analysis. Because the utterances in both the Santa Barbara Corpus and CHILDES were significantly shorter than the sentences in the British National Corpus, we analyzed all utterances of at least 5 and most 15 words (see Fig. \ref{fig:spoken_figs}(A)). Models were estimated separately for each of the 3 corpora.

## Results and Discussion

The information curves found in adults-adult utterances were quite similar to those of parent-child utterances and child-parent utterances (Fig. \ref{fig:spoken_figs}(B)). Under the unigram model, information rose steeply in the beginnings of utterances, was relatively flatter in the middle of utterances, and the rose even more steeply at the ends. Under the trigram model, the first parts words of sentences contained the most information, information was relatively constant in the middle of utterances, and then rose slightly again at the ends. 

Unfortunately, we we cannot compare the amount of information in each word across corpora--surprisal is highly correlated with corpus size (e.g. there is less information in adults' speech in Santa Babara than in children's speech in CHIDLES). However, we can compare the shapes of these curves both to each other and to the written English sentences in Study 1 \ref{fig:bnc_plots}(B). All of these curves appeared to share their important qualitative features, including the sharp rise at the end under the unigram model and the attenuation of this rise under the trigram model. There are small differences--such as the flatter shape in the middle of written sentences than spoken utterances, but this difference is pronounced in the utterances of the Santa Barbara corpus relative to utterances of parents in CHILDES, suggesting that it may be partly a function of corpus size. 

Thus, English--both writen and spoken, both produced by adults and by children-appears to have a characteristic shape. Are the features of this shape features of English, or features of language more broadly? In Study 3 we apply this technique to a diverse set of written languages of different families to ask whether these structures vary cross-linguisticly. 

```{r spoken_figs, fig.env = "figure*", fig.pos = "tb", fig.height = 4.5, fig.width = 7, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "(A) The distribution of sentence lengths in the spoken English corpora: Adults in Santa Barbara, and parents and children in CHILDES. We analyzed sentences of length 5-15 (colored). (B) Charateristic surprisal curves for these corpora."}
SPOKEN_MIN <- 5
SPOKEN_MAX <- 15

# raw length distribution
spoken_plot1 <- lengths %>%
    filter(language != "bnc", length <= 40) %>%
    mutate(corpus = case_when(language == "sbc" ~ "adults",
                              source == "adult" ~ "parents",
                              T ~ "children"),
           corpus = factor(corpus, levels = c("adults", "parents",
                                              "children")),
           group = if_else(length < SPOKEN_MIN | length > SPOKEN_MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    facet_wrap(~ corpus, scales = "free_y") +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray")) + 
    labs(tag = "A")



spoken_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.25, 1.75),
                          plot_person = c("adults", "adults")) %>%
    mutate(plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children")))

spoken_plot2 <- tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), 
           language %in% c("sbc", "childes")) %>% 
    mutate(plot_person = case_when(language == "sbc" ~ "adults",
                                   source == "adult" ~ "parents",
                                   T ~ "children"),
           plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children"))) %>%
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    facet_wrap(~ plot_person) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = spoken_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("barycenter coordinate") + 
    ylab("surprisal") + 
    scale_color_colorblind() + 
    theme(legend.position = "none", aspect.ratio = 1) +
    labs(tag = "B")

gridExtra::grid.arrange(spoken_plot1, spoken_plot2, ncol = 1)
#cowplot::plot_grid(spoken_plot1, spoken_plot2, axis = 'b', nrow = 1)
```


# Experiment 3: Large-scale data and linguistic features 

We pulled corpora for $159$ diverse languages from Wikipedia, an online general knowledge repository with separate repositories for each of hundreds of languages. We filtered our selection of language corpora on Wikipedia to those which had at least $10,000$ articles. We used the Wikiextractor tool from GitHub [WIKIEXTRACTOR CITATION] to retrieve the dumps of entire Wikipedia archives, then lowercased, removed punctuation and split the corpora into sentences. We trained and tested a model on each language's Wikipedia corpus independently, constructing unigram and trigram surprisals for each language separately. We create frequency-based and contextual surprisal curves for each corpus. The distribution of sentence lengths in Wikipedia corpora tend to resemble the distribution of sentence lengths in the BNC (another written corpus). 

```{r dendro, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
b %>% 
    filter(source == "wikipedia", gram == "unigram") %>%
    select(-source, -gram) %>%
    slice(1:50) %>%
    column_to_rownames("language") %>% 
    dist() %>% 
    hclust() %>%
    plot()
```

```{r wals-plot, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
cosines %>%
    left_join(wals) %>%
    filter(complete.cases(.)) %>%
    ggplot(aes(x = cosine, y = wals_dist, color = gram)) + 
        geom_point(alpha = .5) + 
        geom_smooth(method = "lm")
```


```{r swadesh-plot, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
cosines %>%
    left_join(swadesh) %>%
    filter(complete.cases(.)) %>%
    ggplot(aes(x = cosine, y = ldn, color = gram)) + 
        geom_point(alpha = .5) + 
        geom_smooth(method = "lm")
```

See Figure [Diff languages plot] for examples of frequency-based languages from several different language families. We see that the frequency-based curve for German resembles the English curve we found in Experiments 1 and 2 with a three-step distribution, while the Japanese curve looks very different from all the other curves. The Slavic language curves (for Serbian, Russian and Slovak) resemble each other. 

Regardless of the shape of their respective unigram curves, the trigram curves for all the languages look the same as the English curve. 


```{r language-family, eval = FALSE}
features <- read_csv(here("Data/final_features.csv")) %>%
    pivot_longer(-language, names_to = "feature", values_to = "value")

slopes <- read_csv(here("Data/5barycenters.csv"))
iso_codes <- read_csv(here("Data/wiki/codes.csv"))
families <- read_csv(here("Data/wiki/wals_dataset/language.csv"))

FEATURE <- "x81a"

scaled_b <- tidy_b %>%
    group_by(language, gram) %>%
    mutate(surprisal = scale(surprisal, scale = FALSE))

## match up language curves with their language families
family_curves <- features %>% 
    filter(feature == FEATURE, !is.na(value)) %>%
    left_join(iso_codes, by = "language") %>%
    select(language, iso, value) %>% 
    inner_join(scaled_b, by = "language") %>% 
    inner_join(families %>% select(iso_code, family), by = c("iso" = "iso_code")) %>% 
    distinct()## deduplicate--WALS has some duplicate entries but doesn't affect families

## construct sample curves and bootstrap
family_bootstraps <- family_curves %>% 
  select(-iso, -source) %>%
  group_by(value, gram, position) %>%
  tidyboot_mean(surprisal, nboot = 10) %>%
  ungroup()

## plotting the sample curves
family_bootstraps %>%
  ggplot(aes(x = position, y = empirical_stat, group = value, color = value)) + 
    geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = position_dodge(.25)) +
    geom_line() + 
    facet_wrap(~ gram) + 
    xlab("Word position") + 
    ylab("Average Information") + 
    theme(axis.text.x = element_blank()) #+ 
    #scale_color_manual(values = c("#8b0000"))

# ## why do the unigram curves look so flat in comparison?
# family_slopes %>% 
#   filter(gram == "Unigram") %>% 
#   group_by(family)
```

```{r diff-languages-plot, fig.height=3, fig.width = 3.5, fig.pos = "tb", fig.cap="Characteristic curves for a sample of languages from Wikipedia"}
families <- read_csv(here("Data/Paper/families.csv"))

scaled_b <- tidy_b %>%
    group_by(language, gram) %>%
    mutate(surprisal = scale(surprisal, scale = FALSE))

scaled_b %>%
    left_join(families, by = c("language")) %>%
    filter(language %in% c("Afrikaans", "Czech", 
                           "Japanese", "Tamil", "Kannada",
                           "Hebrew", "Somali", "Maltese")) %>%
    ggplot(aes(x = position, y = surprisal, color = language, label = language,
               group = interaction(language, family))) +
    facet_grid(gram ~ family, scales = "free_y") +
    geom_point() + 
    geom_line() +
    scale_color_ptol() + 
    geom_dl(method = "smart.grid", cex = .5) + 
    theme_few(base_size = 9) +
    theme(legend.position = "none")
```

To compare languages more rigorously, we used two databases of language similarity features. To target lexical differences between languages, we used the $40$-item Swadesh list [@swadesh1955], retrieved from the ASJP database [@wichmann2016]. The Swadesh list is a well-known method for comparing lexical similarity between languages, by quantifying the similarity between the words on the list for pairs of languages, and is often used to compare genetic relationships between languages. We computed the average normalized Levenshtein distance, a string edit distance measure [LDN; @holman2008] between each pair of our Wikipedia languages. 

We split the WALS features by type of feature: the nominative categories features describe aspects of morphology such as case systems and definite/indefinite articles; word order describes SVO as well as head-modifier word order; nominative syntax describes noun behavior such as possessives and adjectives acting as nouns; clauses describes phrasal and broader sentence syntax; and verb categories describe tense, mood and aspect as well as morphology on the verb. 

As our surprisal metric is a lexical measure, we expect the Levenshtein distance to be high. To describe more structural relationships, we used the World Atlas of Language Structures [WALS; @2013] to describe the morphology, syntax, phonology, etymology and semantics--in short the structures in each language. As WALS is a compiled database from dozens of papers from different authors, most of the features and languages are fairly sparse. We use a iterative imputation algorithm for categorical data Multiple Imputation Multiple Correspondence Analysis [MIMCA; @audigier2017] to fill in the missing features. 

We see that only word order shows a significant correlation with the shape of the frequency-based information curves in the wikipedia corpora. The rest of the features show virtually no correlation with the wikipedia corpora. For the context-based information curves, all categories of WALS features show correlations. However, across all languages, the context-based information curves are flat after the first word or two. 

WHAT ABOUT THE SWADESH LIST? 

```{r cor_data}
cor_data <- left_join(cosines, wals, by = c("language1", "language2")) %>%
    left_join(swadesh, by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy()),
           swadesh_cor = map(data, ~cor.test(.x$cosine, .x$ldn,
                                             use = "pairwise") %>% tidy()),
           dist_cor = map(data, ~cor.test(.x$wals_dist, .x$ldn, 
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    pivot_longer(-gram, names_to = "measure", values_to = "correlation") %>%
    unnest(cols = c(correlation))


type_cor_data <- left_join(cosines, wals_types, 
                           by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram, type) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    unnest(cols = c(wals_cor)) %>%
    ungroup() %>%
    mutate(type = factor(type, levels = c("nom_syntax", "clauses", 
                                          "verb_categories", "phonology", 
                                          "word_order", "nom_categories"),
                         labels = c("nominative syntax", "clauses", 
                                    "verb categories", "phonology", 
                                    "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

```


```{r type_cors, fig.height = 5, fig.cap="Correlations between Information curve shapes and WALS features."}
type_annotations <- tibble(type = c("phonology", "phonology"),
                          estimate = c(-.02, .1),
                          gram = c("unigram", "trigram")) %>%
    mutate(type = factor(type, levels = c("nominative syntax", "clauses", 
                                          "verb categories", "phonology", 
                                          "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

ggplot(type_cor_data, aes(x = type, y = estimate, color = gram)) + 
    geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
                    position = position_dodge(.5)) +
    geom_hline(aes(yintercept = 0), linetype = "dashed") + 
    scale_color_ptol() +
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1,
          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
    labs(x = "", y = "correlation between information and WALS") + 
    geom_text(aes(label = gram), data = type_annotations)
```

By considering data from $159$ diverse languages, we see that the frequency-based information curves display unique information distributions for each language. A small part of this cross-linguistic variation is explicable due to top-down features of the language in question, such as canonical word order. The majority of the variation for each language does not derive from top-down features, but we suspect instead arises from bottom-up choices made by the speakers in each language. 

# Conclusion

In this paper we have proposed a novel method for quantifying information structure in any language. Our method derives the unique distribution of information based on word frequency in the language, irrespective of whether the language in question is spoken or written. This information structure characterizes the earliest utterances in child speech all the way to complex sentences in knowledge base entries by adult writers. The shape of the information distribution in a language is correlated with the canonical word order in a language, but is mainly derived from the individual choices speakers make in communication in a language. 

In communication, due to predictive processing, the variation in these unique frequency-based distributions are washed out. Instead, a uniform distribution of information emerges across languages, which may allow listeners to decode information quickly and at a nearly constant rate. Once they have enough context, listeners optimally decode information. 

We average over the surprisal sequences of each length, which obscures most of the variation in information curve shapes. This may also mean that there's more bottom-up processing. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
