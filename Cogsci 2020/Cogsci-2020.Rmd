---
title: "Speakers communicate using language-specific information distributions"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{klafka@andrew.cmu.edu} and \texttt{yurovsky@cmu.edu} \\
        Department of Psychology \\ Carnegie Mellon University}

abstract: >
    What role does communicative efficiency play in how we organize our utterances? In this paper, we present a novel method of examining how much information speakers in a given language communicate in each word, surveying numerous diverse languages. We find that speakers produce frequent and informative words at regular parts of their utterances, depending on language they use. The information distribution for each language is derived in part from the features and genealogy of the language. This robust information distribution characterizes both spoken and written communication, and emerges in children's earliest utterances. However, in real-time communication, in-context word predictability allows listeners to process information at a constant, optimal rate, regardless of the information distribution in the language they understand.
    
keywords: >
    information theory; communication; language modeling; computational modeling
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache = T, 
                      message=F, sanitize = T)
```

```{r, libraries, include=FALSE}
require(png)
require(grid)
require(xtable)
require(here)
require(feather)
require(tidyverse)
require(tidytext)
library(tidyboot)
library(ggthemes)
library(widyr)
```

```{r preprocess-data}
lengths <- read_csv(here("Data/Paper/lengths.csv"))
# 
# b <- read_csv(here("Data/final_barycenters.csv")) 
# bnc <- read_csv(here("Surprisals/adult/unigram/bnc_compressed.csv"),
#                 col_names = F) %>%
#   mutate(id = 1:n()) %>%
#   pivot_longer(-id, names_to = "position", values_to = "surprisal") %>%
#   filter(complete.cases(.)) %>%
#   mutate(position = str_extract(position, "\\d+"),
#          position = as.numeric(position)) %>%
#   group_by(id) %>%
#   mutate(length = max(position)) %>%
#   ungroup() %>%
#   select(-id)
bnc <- read_csv(here("Data/bnc_lengths.csv"))

b <- read_csv(here("Data/5barycenters.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

# b <- read_csv(here("Data/7barycenters.csv"), 
#               col_names = c("1", "2", "3", "4", "5", "6", "7", 
#                             "language", "source", "gram"))

tidy_b <- b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


non_wiki_b <- read_csv(here("Data/5barycenters_new.csv"),
              col_names = c("1", "2", "3", "4", "5",
                            "language", "source", "gram"))

tidy_non_wiki_b <- non_wiki_b %>%
     pivot_longer(-c(language, source, gram), 
                  names_to = "position", values_to = "surprisal") %>%
     mutate(position = as.numeric(position),
            gram = factor(gram, levels = c("unigram", "trigram")))


# wals <- read_csv(here("Data/final_features.csv"))
wals <- read_csv(here("Data/Paper/wals_distances.csv"))
wals_types <- read_csv(here("Data/Paper/wals_type_distances.csv"))
swadesh <- read_csv(here("Data/Paper/swadesh_distances.csv"))
cosines <- read_csv(here("Data/Paper/cosines.csv")) 

bnc_surprisals <- read_csv(here("Data/Paper/bnc_subset.csv")) %>%
    mutate(gram = factor(gram, levels = c("unigram", "trigram")))

```

```{r theme}
theme_set(theme_few())
```

# Introduction

We use language for a variety of purposes like greeting friends, making records, and signaling group identity. But, these purposes all share a common goal: Transmitting information that changes the mental state of the listerner or reader [@austin1975]. For this reason, language can be thought of as a code, one that allows speakers to turn their intended meaning into a message that can be transmitted to a listener or reader, and subsequently converted by the listener back into an approximation of the intended meaning [@shannon1948]. How should we expect this code to be structured?

If language has evolved to be a code for information transmission, its structure should reflect this process of optimization [@anderson1989]. The optimal code would have to work with two competing pressures: (1) For listeners to easily and successfully decode messages sent by the speaker, and (2) For speakers to easily code their messages and transmit them with minimal effort and error. A fundamental constraint on both of these processes is the linear order of spoken language--sounds are produced one at a time and each is unavailable perceptually once it is no longer being produced. 

Humans accomodate this linear order constraint through incremental processing. People process speech continuously as it arrives, predicting upcoming words and building expectations about the likely meaning of utterances in real-time rather than at their conclusion [@kutas2011; @tanenhaus1995; @pickering2013]. Since prediction errors can lead to severe processing costs and difficulty integrating new information on the part of listeners, speakers should seek to minimize prediction errors. However, the cost of producing more predictable utterances is using more words. Thus, the optimal strategy for speakers seeking to minimize their production costs is to produce utterances that are just at the prediction capacity of listeners without exceeding this capacity [@aylett2004; @genzel2002]. In other words, speakers should maintain a constant rate information of as close to the listener's fastest decoding rate as possible.

This Uniform Information Density hypothesis has found support at a variety of levels of language from the structure of individual words, to the syntactic structure of utterances [@piantadosi2011; @jaeger2007; see @gibson2019 for a review]. Further, speakers make lexical choices that smooth out the information in their utterances [@mahowald2013; @jaeger2007]. However, while speakers can control which of several near-synonyms they produce, or whether to produce an optional complementizer like "that," they cannot control the grammatical properties of their native language like canonical word order that impose top-down constraints on the structure of utterances. While speakers may produce utterances as uniform in information density as their languages will allow, these top-down constraints may impose significant variation. 

How significant are these top-down constraints? One previous paper analyzed the information content in English sentences and found a surprising three-step shape where information first rises, then plateus, and then sharply rises again at the ends of sentences [@yu2016]. We build on these ideas, asking (1) Whether this shape depends on listener's predictive models, (2) Whether this shape varies across linguistic contexts, and (3) Whether this shape is broadly characteristic of a diverse set of languages or varies predictably from language to language. We find that languages are characterized by highly-reliable but cross-linguisticly variable structures that co-vary with typological features, but that predictive coding flattens these shapes across languages, in accord with predictions of the Uniform Information Density hypothesis.

# Study 1: Information in Written English

The Uniform Information Density Hypothesis predicts that people should structure their utterances so that the amount of information in each unit of language remains constant. An influentual early test of this idea was performed by @genzel2002 who analyzed the amount of information in successive paragraphs of the same text. They found that the amount of information increased across paragraphs when each was considered in isolation. They reasoned that since all prior paragraphs provide the context for reading each new paragraph, the amount of total information (context + paragraph) was constant for human readers.

@yu2016 applied this same logic to analysis of the information in individual sentences, computing the entropy of each successive word in an utterance. Surprisingly, they found found a distinctive three-step distribution for information in a corpus of written English. The first word of each sentence tended to contain little information; words in the middle of sentences each contained roughly the same amount of information, nut the final word of each sentence contained much more information than any other word. They found the same distribution across sentence lengths, from sentences with $15$ words to sentences with $45$ words. They took this as evidence against the Uniform Information Density Hypothesis as, unlike Genzel and Charniak's [-@genzel2002] results, information plateud in the middle of sentences rather than increasing as it did at the beginnings and ends.

We replicate their analysis here, bringing it more in line with Genzel and Charniak's [-@genzel2002] methods. While Yu et al. [-@yu2016] considered only the information in each word, we build two different models. The first, replicating their analysis, considers the surprisal of each word read in isolation. The second, following @genzel2002, is a trigram model which considers the surprisal of each word having read the prior two words. We take this trigram model as a better proxy for human reader's processing of these sentences. We also develop a method for averaging the curves for sentences of different lenghts together to provide a single typical information curve signature.

## Corpus

Following @yu2016, we selected the British National Corpus (BNC) for analysis [@british-national-corpus-consortium2007]. The British National Corpus is ~100 million word corpus consisting of spoken (10%) and written (90%) English from the late 20th Century.

## Pre-processing

```{r bnc-setup}
MIN <- 5
MAX <- 45

bnc_total <- lengths %>%
    filter(language == "bnc") %>%
    pull(n) %>%
    sum()

bnc_kept <- lengths %>% 
    filter(length >= MIN, length <= MAX) %>% 
    summarise(n = sum(n)) %>%
    mutate(n = (n/bnc_total) * 100) %>%
    pull(n) %>%
    round(2)
    
```

We began the XML version of the corpus, and used the \texttt{justTheWords.xsl} script provided along with the corpus to produce a text file with one sentence of the corpus on each line. Compound words (like "can't") were combined, and all words were converted to lowercase before analysis. This produced a corpus of just over six million utterance of varying lenghts. From these, we excluded utterances that were too short to allow for reasonable estimation of information shape (fewer than `r MIN` words), and utterances that were unusually long (more than `r MAX` words). This exclusion left us with `r bnc_kept`% of the utterances (Fig \ref{fig:bnc-lengths}.

```{r bnc-lengths, echo = F, fig.height = 3, fig.width = 3, fig.cap="Sentence length distribution in the British National Corpus. Lengths included in analysis are dark."}

# raw length distribution
lengths %>%
    filter(language == "bnc", length <= 100) %>%
    mutate(group = if_else(length < MIN | length > MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray"))

```

## Estimating information

To estimate how information is distributed across utterances, we computed the lexical surprisal of each word under two different models [@levy2008; @shannon1948]. Intuitively, the surprisal of a word is a measure of how unexpected it would be to read that word, and thus how much information it contains. First, following @yu2016, we estimated a unigram model which considers each word independently, asking how unexpected that word would be in the absence of any context: $\text{surprisal}(\text{word}) = -\log P(\text{word})$. This unigram surprisal measure is a direct transformation of the word's frequency and thus less frequent words are more surprising.

Second, we estimated a trigram model in which the surprisal of a given word ($w_i$) encodes how unexpected it is to read it after reading the prior two words ($w_{i-1}$ and $w_{i-2}$): $\text{surprisal}(w_{i}) = -log P(w_i|w_{i-1},w_{i-2})$. This metric encodes the idea that words that are low frequency in isolation (e.g. "meatballs") may become much less surprising in certain contexts (e.g. "spaghetti and meatballs") but more surprising in others (e.g. "coffee with meatballs"). In principle, we would like to encode the surprisal of a word given all of the prior sentential context ($\text{surprisal}(w_{i}) = -P(w_i|w_{i-1}w_{i-2}...w_{1})$. However, the difficulty of correctly estimating these probabilities from a corpus grow combinatorically with the number of prior words, and in practice trigram models perform well as an approximation [see e.g. @chen1999; @smith2013].

### Model details

We estimated the surprisal for each word type in the British National Corpus using the KenLM toolkit [@heafield2013]. Each utterance was padded with a special start-of-sentence word "$\left<s\right>$" and end of sentence word "$\left</s\right>$". Trigram estimates did not cross sentence boundaries, so for example the surprisal of the second word in an utterances was estimated as ($\text{surprisal}(w_{2}) = -P(w_2|w_{i},\left<s\right>)$. 

Naïve trigram models will underestimate the surprisal of words in low-frequency trigrams (e.g. if the word "meatballs" appears only once in the corpus following exactly the words "spaghetti and", it is perfectly predictable from its prior two words). To avoid this underestimation, we used modified Kneser-Ney smoothing which discounts all ngram frequency counts--reducing the impact of rare ngrams on probability calculations--and interpolates lower-order ngrams into the calcuations. These lower-order ngrams are weighted according to the number of distinct contexts they occur as a continuation [e.g. "Francisco" may be a common word in a corpus, but likely only occurs after "San" as in "San Francisco", so it receives a lower weighting; see @chen1999]. 

### Averaging curves

To develop a characteristic information curve for sentences in the corpus, we needed to aggregate sentences that varied dramatically in length (Fig \ref{bnc-lengths}). We used Dynamic Time Warping Barycenter Averaging (DBA), an algorithm for finding the average of sequences that share and underlying pattern but vary in length [@petitjean2011]. DBA is an extension of standard Dynamic Time Warping, which searches for an invariant template in shifted and stretched instances of that template [e.g. all instances of the vowel 'a' in acoustic recordings of a speaker's productions even if the instances vary in their duration and intensity]. DBA inverts this idea, discovering a latent invariant template from a set of sequences. 

We used DBA to discover the short sequence of surprisal values that characterized the surprisal curves common to sentences of varying sentence lengths. We first averaged individual sentences of the same length together, and then applied the DBA algorithm to this set of average sequences. DBA requires a parameter specifying the length of the template sequence. We chose 5 as the length of the template sequence based on our inspection of the curves from the British National Corpus as well as the curves we found in subsequent studies. However, the results of this and the following studies were robust to other choices of this length parameter (7 and 10).

## Results 

```{r bnc_raw_full, eval = F}
bnc_unigram_surprisals <- read_csv(here("Surprisals/adult/unigram/bnc.csv"))
bnc_trigram_surprisals <- read_csv(here("Surprisals/adult/trigram/bnc.csv"))

bnc_subset_unigram <- bnc_unigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "unigram")

bnc_subset_trigram <- bnc_trigram_surprisals %>%
    filter(length %in% c(15, 30, 45)) %>%
    mutate(new_sent = !(position > lag(position)), 
           new_sent = if_else(is.na(new_sent), 0, as.numeric(new_sent)),
           sentence = cumsum(new_sent)) %>%
    group_by(length, position) %>%
    summarise(mean = mean(surprisal), sem = sd(surprisal)/sqrt(n()-1)) %>%
    mutate(gram = "trigram")

write_csv(bind_rows(bnc_subset_unigram, bnc_subset_trigram),
          here("Data/Paper/bnc_subset.csv"))

```

```{r bnc_raw, fig.cap="Surprisal by sentence position in the British National Corpus. Error bars indicate 95\\% confidence intervals (sometimes invisible because of precision)"}
bnc_annotations <- tibble(position = c(12, 27, 42),
                          length = c(15, 30, 45),
                          gram = c("unigram", "unigram", "unigram"),
                          mean= c(4, 4, 4))

ggplot(bnc_surprisals, aes(x = position, y = mean, color = as.factor(length),
                           linetype = gram, label = length)) + 
    geom_errorbar(aes(ymin = mean - 1.96 * sem, ymax = mean + 1.96 * sem), width = 0) + 
    geom_line(position = position_dodge(.5)) + 
    scale_color_ptol() + 
    theme(legend.position = "none") + 
    scale_linetype_manual(values = c("solid", "longdash")) + 
    geom_text(data = bnc_annotations) + 
    annotate("text", x = 6.5, y = 3.5, label = "Unigram") + 
    annotate("text", x = 6.5, y = 2.5, label = "Trigram") + 
    labs(x = "Sentence position", y = "Surprisal") + 
    theme(aspect.ratio = 1)
```


```{r bnc_barycenters, fig.cap="Charateristic surprisal curves for the British National Corpus"}
bnc_b_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.5, 2))

tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), language %in% c("bnc")) %>% 
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = bnc_b_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("Coordinate") + 
    ylab("Surprisal") + 
    scale_color_ptol() + 
    theme(aspect.ratio = 1, legend.position = "none") 
```


We replicate Yu et al.'s [-@yu2016] result using the surprisal metric in place of the entropy metric. We use the frequency-based or “contextless” surprisal metric, which determines the average distribution of information based on word frequencies in a corpus. A priori we expect that the frequency-based metric will produce a flat distribution of information across word positions in the BNC. We find the same frequency-based information trajectory as Yu et al. with little information in the first words of utterances and the most information in the final word, see Figure \ref(fig:bnc_raw). 

We now include two words of context (trigrams) for each word in our measurements. We observe a flattening effect of context for both spoken and written English. After the first word or two, where the listener does not have access to prior context, then they decode information at a flat and more or less uniform rate. The contextual information curve for the BNC is in Figure \@ref(fig:bnctrigrams).

We now show that spoken English from adults, parents and children all follows the same characteristic frequency-based and context-based information distributions as written English. English as a whole is characterized by the three-step frequency-based distribution, and context enables the listener or reader to process English at a nearly constant rate. 

# Experiment 2: English in Other contexts

```{r spoken_figs, fig.env = "figure*", fig.pos = "tb", fig.height = 4.5, fig.width = 7, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "(A) Sentence length distributions in the spoken language Corpora: Adults in Santa Barbara, and Parents and children in CHILDES. (B) Charateristic surprisal curves for these corpora."}
SPOKEN_MIN <- 5
SPOKEN_MAX <- 15

# raw length distribution
spoken_plot1 <- lengths %>%
    filter(language != "bnc", length <= 40) %>%
    mutate(corpus = case_when(language == "sbc" ~ "adults",
                              source == "adult" ~ "parents",
                              T ~ "children"),
           corpus = factor(corpus, levels = c("adults", "parents",
                                              "children")),
           group = if_else(length < SPOKEN_MIN | length > SPOKEN_MAX, "out", "in")) %>%
    ggplot(aes(x = length, y = n, fill = group, color = group)) +
    facet_wrap(~ corpus, scales = "free_y") +
    geom_col() +
    labs(x = "sentence length", y = "frequency") +
    theme(legend.position = "none", aspect.ratio = 1) + 
    scale_fill_manual(values = c(ptol_pal()(1), "light gray")) +
    scale_color_manual(values = c(ptol_pal()(1), "light gray")) + 
    labs(tag = "A")



spoken_annotations <- tibble(position = c(2.5, 2.5),
                          gram = c("unigram","trigram"),
                          surprisal = c(3.25, 1.75),
                          plot_person = c("adults", "adults")) %>%
    mutate(plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children")))

spoken_plot2 <- tidy_non_wiki_b %>% 
    filter(gram %in% c("unigram", "trigram"), 
           language %in% c("sbc", "childes")) %>% 
    mutate(plot_person = case_when(language == "sbc" ~ "adults",
                                   source == "adult" ~ "parents",
                                   T ~ "children"),
           plot_person = factor(plot_person, levels = c("adults", "parents",
                                              "children"))) %>%
    ggplot(aes(x = position, y = surprisal, color = gram)) + 
    facet_wrap(~ plot_person) + 
    geom_point(size = .75) + 
    geom_line() + 
    geom_text(aes(label = gram), data = spoken_annotations) +
    scale_x_continuous(breaks = 1:5) + 
    xlab("barycenter coordinate") + 
    ylab("surprisal") + 
    scale_color_ptol() + 
    theme(legend.position = "none", aspect.ratio = 1) +
    labs(tag = "B")

gridExtra::grid.arrange(spoken_plot1, spoken_plot2, ncol = 1)
#cowplot::plot_grid(spoken_plot1, spoken_plot2, axis = 'b', nrow = 1)
```

Spoken language and written language diverse in a number of respects. Speech in general is produced and therefore can only be processed at a slower rate than the written word. Speech occurs in a multimodal environment, along with other features such as prosody and visual information such as gesture and information from the environment. The words in speech are often different from the set of words writers draw from in a particular language: writers do not have to obey the same cost of production or the now-or-never bottleneck in comprehension [CITE CHRISTIANSEN and CHATER]. See Figure [WHICH FIGURE] for the distribution of word lengths in the Santa Barbara corpus of spoken conversations between adult strangers [@sbc]. Compared to Figure 

Child language input before children learn to read [AT WHAT AGE] is entirely spoken, and children's main interlocutors are their parents. In particular, the language input children receive may differ in terms of the vocabulary, rate of speech, length of words and syntax from adult speech [CDS CITATION]. In North American English, speech to children is often simplified [CDS CITATION]. In return, child speech is simplified: children typically do not speak their first multiword utterances until their second birthday [IS THIS RIGHT?]. Much complex children's language input actually comes from children's books [CITE Jessica Montag]. See Figure [WHICH FIGURE] for a typical distribution of word lengths in child-directed and child speech. The vast majority of utterances are only a few words long.  

In this context, we will show that despite the differences in utterance length, vocabulary and grammatical complexity between spoken and written English, we observe the same information trajectory in both modalities. For spoken English, we use the Santa Barbara corpus of spoken English telephone conversations [@sbc]. We use the North American English collection of corpora from CHILDES [@macwhinney2000], which includes `1.07` million child utterances mainly before the age of five, and `1.7` million parent child-directed utterances. We apply a similar cleaning procedure to the one we used for the BNC. We lowercase and remove punctuation from all utterances in the corpora. To obtain the CHILDES utterances, we use childesr [childesr citation], a frontend in the R programming language [CITE R core team]. 

<!-- We use spoken  [@macwhinney2000] to capture the developmental picture. We use corpora for Spanish, German, French, Mandarin Chinese and Japanese as well as English. Mandarin and Japanese are not natively written using the Latin alphabet, and moreover words are not segmented in their native scripts. Instead of the native scripts, we use transliterations from the corpus for each of the Mandarin and Japanese utterances into pinyin for Mandarin and romanji for Japanese. In these transliterations, words are previously segmented. 

We observe a distinct and characteristic frequency-based information trajectory for each language, robust across each utterance length. We see the same distribution of information for both parents and children. The parent often has more information on average at each word position in their utterances. This is an effect of the surprisal metric: parents speak more utterances than their children in most of the corpora, which inflates the number of tokens they use and increases the surprisal of hearing a rare word. We include the frequency-based information curve from the North American English CHILDES collection for comparison. See Figure \@ref(fig:childesunigrams)-->

<!-- English, Spanish, French and German feature similar information curve shapes, with slight variations. The German information curve features lower information for longer towards the beginnings of utterances, possibly due to the grammatical restriction that the second word in German utterances must be a verb (V2). Spanish features a larger spike in the amount of information in the final word of utterances. For Japanese and Mandarin, we observe completely different frequency-based information curve trajectories. The Japanese frequency-based information curve trajectory begins high and finishes low, the mirror image of the German and Romance language information curves. The Mandarin curve begins low and finishes low, but features high information in the middle of utterances. We hypothesize this may be due to Japanese and Mandarin speakers typically ending their utterances with particles, which are common and thus contain little information on their own.   -->

We observe the exact same frequency-based distribution for our information curves in adult conversation, parent speech to children and children's speech as in spoken English. This indicates that the three-step frequency-based distribution in English holds for all modalities and ages in the language. Children, from their first multiword utterances, tend to produce words according to the information distribution we have found, and continue following that distribution when they grow up and speak to other adults. Final words in English are important in child-directed speech [CITE ASLIN PAPER]. 

We observe that the contextual distribution is likewise similar to the distribution we found for the BNC: high and then immediately flattening out once the listener has a word or two of context to predict the word their interlocutor will produce next. 

This only captures the picture for English. In our next experiment, we find unique frequency-based distributions for each language, which are determined in part by top-down features in the language such as word order. We will show a similar contextual smoothing effect, where readers and listeners in each language are able to decode information at a constant rate with only a couple of words of context. 

# Experiment 3: Large-scale data and linguistic features 

We pulled corpora for $159$ diverse languages from Wikipedia, an online general knowledge repository with separate repositories for each of hundreds of languages. We filtered our selection of language corpora on Wikipedia to those which had at least $10,000$ articles. We used the Wikiextractor tool from GitHub [WIKIEXTRACTOR CITATION] to retrieve the dumps of entire Wikipedia archives, then lowercased, removed punctuation and split the corpora into sentences. We trained and tested a model on each language's Wikipedia corpus independently, constructing unigram and trigram surprisals for each language separately. We create frequency-based and contextual surprisal curves for each corpus. The distribution of sentence lengths in Wikipedia corpora tend to resemble the distribution of sentence lengths in the BNC (another written corpus). 

```{r dendro, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
b %>% 
    filter(source == "wikipedia", gram == "unigram") %>%
    select(-source, -gram) %>%
    slice(1:50) %>%
    column_to_rownames("language") %>% 
    dist() %>% 
    hclust() %>%
    plot()
```

```{r wals-plot, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
cosines %>%
    left_join(wals) %>%
    filter(complete.cases(.)) %>%
    ggplot(aes(x = cosine, y = wals_dist, color = gram)) + 
        geom_point(alpha = .5) + 
        geom_smooth(method = "lm")
```


```{r swadesh-plot, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES context-based trigram curves", eval = F}
cosines %>%
    left_join(swadesh) %>%
    filter(complete.cases(.)) %>%
    ggplot(aes(x = cosine, y = ldn, color = gram)) + 
        geom_point(alpha = .5) + 
        geom_smooth(method = "lm")
```

See Figure [Diff languages plot] for examples of frequency-based languages from several different language families. We see that the frequency-based curve for German resembles the English curve we found in Experiments 1 and 2 with a three-step distribution, while the Japanese curve looks very different from all the other curves. The Slavic language curves (for Serbian, Russian and Slovak) resemble each other. 

Regardless of the shape of their respective unigram curves, the trigram curves for all the languages look the same as the English curve. 

```{r diff-languages-plot, echo = F, fig.height=2, fig.width=3.5, fig.cap="CHILDES frequency-based and context-based curves", eval = F}

tidy_b %>%
    filter(language %in% c("German", "Serbian", "Japanese", "Catalan", "Russian", "Slovak"),
           source == "wikipedia") %>%
    ggplot(aes(x = position, y = surprisal, color = language)) +
    facet_wrap(~ gram) +
    geom_point() + 
    geom_line()
```

To compare languages more rigorously, we used two databases of language similarity features. To target lexical differences between languages, we used the $40$-item Swadesh list [@swadesh1955], retrieved from the ASJP database [@wichmann2016]. The Swadesh list is a well-known method for comparing lexical similarity between languages, by quantifying the similarity between the words on the list for pairs of languages, and is often used to compare genetic relationships between languages. We computed the average normalized Levenshtein distance, a string edit distance measure [LDN; @holman2008] between each pair of our Wikipedia languages. 

We split the WALS features by type of feature: the nominative categories features describe aspects of morphology such as case systems and definite/indefinite articles; word order describes SVO as well as head-modifier word order; nominative syntax describes noun behavior such as possessives and adjectives acting as nouns; clauses describes phrasal and broader sentence syntax; and verb categories describe tense, mood and aspect as well as morphology on the verb. 

As our surprisal metric is a lexical measure, we expect the Levenshtein distance to be high. To describe more structural relationships, we used the World Atlas of Language Structures [WALS; @2013] to describe the morphology, syntax, phonology, etymology and semantics--in short the structures in each language. As WALS is a compiled database from dozens of papers from different authors, most of the features and languages are fairly sparse. We use a iterative imputation algorithm for categorical data Multiple Imputation Multiple Correspondence Analysis [MIMCA; @audigier2017] to fill in the missing features. 

We see that only word order shows a significant correlation with the shape of the information curves in the wikipedia corpora. The rest of the features show virtually no correlation with the wikipedia corpora. 

WHAT ABOUT THE SWADESH LIST? 

```{r cor_data}
cor_data <- left_join(cosines, wals, by = c("language1", "language2")) %>%
    left_join(swadesh, by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy()),
           swadesh_cor = map(data, ~cor.test(.x$cosine, .x$ldn,
                                             use = "pairwise") %>% tidy()),
           dist_cor = map(data, ~cor.test(.x$wals_dist, .x$ldn, 
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    pivot_longer(-gram, names_to = "measure", values_to = "correlation") %>%
    unnest(cols = c(correlation))


type_cor_data <- left_join(cosines, wals_types, 
                           by = c("language1", "language2")) %>%
    filter(language1 < language2) %>%
    group_by(gram, type) %>%
    nest() %>%
    mutate(wals_cor = map(data, ~cor.test(.x$cosine, .x$wals_dist,
                                          use = "pairwise") %>% tidy())) %>%
    select(-data) %>%
    unnest(cols = c(wals_cor)) %>%
    ungroup() %>%
    mutate(type = factor(type, levels = c("nom_syntax", "clauses", 
                                          "verb_categories", "phonology", 
                                          "word_order", "nom_categories"),
                         labels = c("nominative syntax", "clauses", 
                                    "verb categories", "phonology", 
                                    "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

```


```{r type_cors, fig.height = 5, fig.cap="Correlations between Information curve shapes and WALS features."}
type_annotations <- tibble(type = c("phonology", "phonology"),
                          estimate = c(-.02, .1),
                          gram = c("unigram", "trigram")) %>%
    mutate(type = factor(type, levels = c("nominative syntax", "clauses", 
                                          "verb categories", "phonology", 
                                          "word order", "nominative categories")),
           gram = factor(gram, levels = c("unigram", "trigram")))

ggplot(type_cor_data, aes(x = type, y = estimate, color = gram)) + 
    geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
                    position = position_dodge(.5)) +
    geom_hline(aes(yintercept = 0), linetype = "dashed") + 
    scale_color_ptol() +
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1,
          axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
    labs(x = "", y = "correlation between information and WALS") + 
    geom_text(aes(label = gram), data = type_annotations)
```

# Conclusion

In this paper we did model and it showed unique distributions for unigrams and same distribution for trigrams. Developmental angle. 

Follow-up. Possible questions one might have. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
