---
title: "Mutual Information"
author: "Josef Klafka and Daniel Yurovsky"
date: "3/17/2020"
output: html_document
---

```{r setup, include=FALSE}
require(tidyverse)
require(here)
require(janitor)
require(widyr)

knitr::opts_chunk$set(echo = TRUE)
```

```{r read-data, message = FALSE}
data <- read_csv(here("../Data/5barycenters.csv")) %>%
  filter(source == "wikipedia") %>%
  left_join(read_csv(here("../Data/final_features.csv")), by = "language") %>%
  clean_names()
```

## Our approach

We compute pairwise mutual information between each coordinate of the barycenter and each WALS feature. We use the metric described in "Mutual Information between Discrete and Continuous Data Sets" by Brian C. Ross published in PLoS One. From now on, we refer to this metric as KNN-MI (k-nearest-neighbors mutual information).

The metric is based on the k-nearest-neighbors algorithm in supervised machine learning. From the paper:

"For each data point i our method computes a number $I_i$ based on its nearest-neighbors in the continuous variable $Y$. We first find the $k$th-closest neighbor to point $i$ among those $N_{x_i}$ data points whose value of the discrete variable equals $x_i$ using some distance metric of our choice. Define $d$ as the distance to this $k$th neighbor. We then count the number of neighbors $m_i$ in the full data set that lie within distance $d$ to point $i$ (including the kth neighbor itself). Based on $N_{x_i}$ and $m_i$ we compute

$$I_i = \psi(N) - \psi(N_{x_i}) + \psi(k) - \psi(m_i)$$

where $\psi(\cdot)$ is the digamma function. To estimate the [mutual information] from
our data set, we average $I_i$ over all data points.

$$I(X, Y) = \langle I_i \rangle = \psi(N) - \langle\psi(N_{x_i})\rangle + \psi(k) - \langle\psi(m_i)\rangle$$

In our implementation $k$ is some fixed (low) integer of the userâ€™s choice; larger k-values lead to lower sampling error but higher coarse-graining error."


```{r compute-pair, eval = F}
## for nearest neighbors
k <- 3

# get only the first coordinate of the unigram barycenter and WALS feature x103a
single_feature <- data %>%
  filter(gram == "unigram") %>%
  select(x1, x103a) %>%
  mutate(point = 1:n())

# how languages have each feature value
single_feature_Nxs <- single_feature %>%
  group_by(x103a) %>%
  summarise(Nx = n())

# pairwise distances between each
single_feature_all_dists <- single_feature %>%
  mutate(identity = 1) %>%
  pairwise_dist(point, identity, x1, method = "manhattan", upper = TRUE)

single_feature_group_dists <- single_feature %>%
  ## what's the closest data point, get its distance
  pairwise_dist(point, x103a, x1, method = "manhattan", upper = TRUE) %>%
  group_by(item1) %>%
  arrange(distance) %>%
  slice(k) %>%
  ungroup() %>%
  select(-item2) %>%
  left_join(select(single_feature, point, x103a),
            by = c("item1" = "point")) %>%
  rename(min_dist = distance)


single_feature_mis <- single_feature_all_dists %>%
  left_join(single_feature_group_dists, by = "item1") %>%
  filter(distance <= min_dist) %>%
  group_by(item1) %>%
  summarise(mi = n())

single_feature_mi_data <- single_feature_group_dists %>%
  mutate(N = n(), k = k) %>%
  left_join(single_feature_Nxs, by = "x103a") %>%
  left_join(single_feature_mis, by = "item1")


single_feature_Is = single_feature_mi_data %>%
  mutate_at(vars(N, k, Nx, mi), digamma) %>%
  mutate(Ii = N - Nx + k - mi) %>%
  summarise(I = mean(Ii))

```

All features
```{r compute-pair, eval = F}
k <- 3

features <- data %>%
  pivot_longer(cols = x1a:x120a, names_to = "feature", values_to = "feature_value") %>%
  pivot_longer(cols = x1:x5, names_to = "position", values_to = "position_value") %>%
  group_by(gram, feature, position) %>%
  mutate(point = 1:n(),
         identity = 1)

N <- max(pull(features, point))

Nxs <- features %>%
  group_by(feature_value, add = TRUE) %>%
  summarise(Nx = n())

all_dists <- features %>%
  ungroup() %>%
  filter(feature == "x1a") %>%
  group_by(gram, position) %>%
  nest() %>%
  mutate(dist = map(data, ~pairwise_dist(., point, identity, position_value,
                                         method = "manhattan", upper = TRUE))) %>%
  select(-data) %>%
  unnest(cols = c(dist))

all_group_dists <- features %>%
  nest() %>%
  group_by(gram, feature, position) %>%
  mutate(dist = map(data, ~pairwise_dist(., point, feature_value,
                                         position_value, method = "manhattan",
                                         upper = TRUE))) %>%
  select(-data) %>%
  unnest(cols = c(dist)) %>%
  group_by(item1, add = TRUE) %>%
  arrange(distance) %>%
  slice(k) %>% # or whatever you want k to be
  ungroup() %>%
  select(-item2) %>%
  left_join(select(features, gram, feature, feature_value, position, point),
            by = c("gram", "feature", "position", "item1" = "point")) %>%
  rename(min_dist = distance)


mis <- all_dists %>%
  full_join(all_group_dists, by = c("gram", "position", "item1")) %>%
  filter(distance <= min_dist) %>%
  group_by(gram, position, feature, item1) %>%
  summarise(mi = n())

mi_data <- all_group_dists %>%
  mutate(N = N, k = k) %>%
  left_join(Nxs, by = c("gram", "feature", "position", "feature_value")) %>%
  left_join(mis, by = c("gram", "feature", "position", "item1"))


Is = mi_data %>%
  mutate_at(vars(N, k, Nx, mi), digamma) %>%
  mutate(Ii = N - Nx + k - mi) %>%
  group_by(gram, position, feature) %>%
  summarise(I = mean(Ii))
```
