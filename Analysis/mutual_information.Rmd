---
title: "Mutual Information"
author: "Josef Klafka and Daniel Yurovsky"
date: "3/17/2020"
output: html_document
---

```{r setup, include=FALSE}
require(tidyverse)
require(here)
require(janitor)
require(widyr)
require(ggridges)
library(ggthemes)
library(lmerTest)
library(broom)
library(broom.mixed)

knitr::opts_chunk$set(echo = TRUE)

theme_set(theme_few() + theme(legend.position = "none"))
```

```{r read-data, message = FALSE}
data <- read_csv(here("Data/5barycenters.csv")) %>%
  filter(source == "wikipedia") %>%
  left_join(read_csv(here("Data/final_features.csv")), by = "language") %>%
  clean_names()
```

## Our approach

We compute pairwise mutual information between each coordinate of the barycenter and each WALS feature. We use the metric described in "Mutual Information between Discrete and Continuous Data Sets" by Brian C. Ross published in PLoS One. From now on, we refer to this metric as KNN-MI (k-nearest-neighbors mutual information).

The metric is based on the k-nearest-neighbors algorithm in supervised machine learning. From the paper:

"For each data point i our method computes a number $I_i$ based on its nearest-neighbors in the continuous variable $Y$. We first find the $k$th-closest neighbor to point $i$ among those $N_{x_i}$ data points whose value of the discrete variable equals $x_i$ using some distance metric of our choice. Define $d$ as the distance to this $k$th neighbor. We then count the number of neighbors $m_i$ in the full data set that lie within distance $d$ to point $i$ (including the kth neighbor itself). Based on $N_{x_i}$ and $m_i$ we compute

$$I_i = \psi(N) - \psi(N_{x_i}) + \psi(k) - \psi(m_i)$$

where $\psi(\cdot)$ is the digamma function. To estimate the [mutual information] from
our data set, we average $I_i$ over all data points.

$$I(X, Y) = \langle I_i \rangle = \psi(N) - \langle\psi(N_{x_i})\rangle + \psi(k) - \langle\psi(m_i)\rangle$$

In our implementation $k$ is some fixed (low) integer of the userâ€™s choice; larger k-values lead to lower sampling error but higher coarse-graining error."

## For one WALS feature and one barycenter coordinate

```{r compute-pair, eval = F}
## for nearest neighbors
k <- 3

# get only the first coordinate of the unigram barycenter and WALS feature x103a
single_feature <- data %>%
  filter(gram == "unigram") %>%
  select(x1, x103a) %>%
  mutate(point = 1:n())

# how languages have each feature value
single_feature_Nxs <- single_feature %>%
  group_by(x103a) %>%
  summarise(Nx = n())

# pairwise distances between each
single_feature_all_dists <- single_feature %>%
  mutate(identity = 1) %>%
  pairwise_dist(point, identity, x1, method = "manhattan", upper = TRUE)

# compute the distance of the kth closest language for each coordinate with the same feature value
single_feature_group_dists <- single_feature %>%
  ## what's the closest data point, get its distance
  pairwise_dist(point, x103a, x1, method = "manhattan", upper = TRUE) %>%
  group_by(item1) %>%
  arrange(distance) %>%
  slice(k) %>%
  ungroup() %>%
  select(-item2) %>%
  left_join(select(single_feature, point, x103a),
            by = c("item1" = "point")) %>%
  rename(min_dist = distance)


single_feature_mis <- single_feature_all_dists %>%
  left_join(single_feature_group_dists, by = "item1") %>%
  filter(distance <= min_dist) %>%
  group_by(item1) %>%
  summarise(mi = n())

single_feature_mi_data <- single_feature_group_dists %>%
  mutate(N = n(), k = k) %>%
  left_join(single_feature_Nxs, by = "x103a") %>%
  left_join(single_feature_mis, by = "item1")


single_feature_Is <- single_feature_mi_data %>%
  mutate_at(vars(N, k, Nx, mi), digamma) %>%
  mutate(Ii = N - Nx + k - mi) %>%
  summarise(I = mean(Ii))

```

## For all features and all coordinates

Without any adjustment, some of the mutual information measures produce negative mutual information. For the classic mutual information measure on two discrete or two continuous random variables, the measure is naturally non-negative. One random variable can only at worst give no information about the value of another random variable. Negative mutual information is otherwise only a property of multivariate mutual information. 

To counteract this, we introduce a simple fix: any negative mutual information values for individual WALS feature values are replaced by $0$. 

```{r compute all pairs, eval = F}
k <- 3

# get each feature for each language and each barycenter coordinate
features <- data %>%
  pivot_longer(cols = x1a:x120a, names_to = "feature", values_to = "feature_value") %>%
  pivot_longer(cols = x1:x5, names_to = "position", values_to = "position_value") %>%
  group_by(gram, feature, position) %>%
  mutate(point = 1:n(),
         identity = 1)

# number of languages
N <- max(pull(features, point))

# number of languages with each value for each feature
Nxs <- features %>%
  group_by(feature_value, add = TRUE) %>%
  summarise(Nx = n())

# just for WALS feature 1A:
# compute the distance of the kth closest language for each coordinate with the same feature value
all_dists <- features %>%
  ungroup() %>%
  filter(feature == "x1a") %>%
  group_by(gram, position) %>%
  nest() %>%
  mutate(dist = map(data, ~pairwise_dist(., point, identity, position_value,
                                         method = "manhattan", upper = TRUE))) %>%
  select(-data) %>%
  unnest(cols = c(dist))

# now repeat for all features, with k as set at the top of this section
all_group_dists <- features %>%
  nest() %>%
  group_by(gram, feature, position) %>%
  mutate(dist = map(data, ~pairwise_dist(., point, feature_value,
                                         position_value, method = "manhattan",
                                         upper = TRUE))) %>%
  select(-data) %>%
  unnest(cols = c(dist)) %>%
  group_by(item1, add = TRUE) %>%
  arrange(distance) %>%
  slice(k) %>% # or whatever you want k to be
  ungroup() %>%
  select(-item2) %>%
  left_join(select(features, gram, feature, feature_value, position, point),
            by = c("gram", "feature", "position", "item1" = "point")) %>%
  rename(min_dist = distance)

# how many total points in that neighborhood
mis <- all_dists %>%
  full_join(all_group_dists, by = c("gram", "position", "item1")) %>%
  filter(distance <= min_dist) %>%
  group_by(gram, position, feature, item1) %>%
  summarise(mi = n())

# join the N, k and Nx information
mi_data <- all_group_dists %>%
  mutate(N = N, k = k) %>%
  left_join(Nxs, by = c("gram", "feature", "position", "feature_value")) %>%
  left_join(mis, by = c("gram", "feature", "position", "item1"))

# get the mutual information with the digamma formula
Is <- mi_data %>%
  mutate_at(vars(N, k, Nx, mi), digamma) %>%
  mutate(Ii = N - Nx + k - mi) %>% #,
         #Ii = if_else(Ii > 0, Ii, 0)) %>% # replace negative mutual information with 0
  group_by(gram, position, feature) %>%
  summarise(I = mean(Ii))
```

```{r feature-types}
tidy_Is <- Is %>%
  ungroup() %>%
  mutate(position = as.numeric(gsub("x", "", position)),
         feature = gsub("x", "", feature),
         feature = gsub("a", "A", feature))

wals_types <- read_csv(here("Data/feature_types.csv"))

type_Is <- left_join(tidy_Is, wals_types, by = c("feature"))

ggplot(type_Is, aes(x = I, y = as.factor(position), color = position, fill = position)) + 
  facet_grid(gram ~ type) + 
  geom_density_ridges(aes(alpha = .1)) + 
  geom_vline(aes(xintercept = 0), linetype = "dashed")

type_model <- lmer(I ~ as.factor(position) * type + (1|feature),
                   data = type_Is %>% filter(gram == "unigram")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect, -df)


type_model <- lmer(I ~ type + as.factor(position) + (1|feature),
                   data = type_Is %>% filter(gram == "trigram")) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect, -df)

```



# which features have the most mutual information?
Is %>% 
  ungroup() %>% 
  filter(gram == "unigram", I > 0) %>% 
  arrange(desc(I))
```
