---
title: "Wikipedia information curve analysis"
author: "Josef Klafka and Daniel Yurovsky"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
require(tidyverse)
require(lsa)
require(broom)
library(broom.mixed)
require(janitor)
require(feather)
require(here)
require(lingtypology)
require(lme4)
library(entropy)
require(directlabels)

knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
distances <- read_feather(here("Data/Wikipedia/all_spaces.feather"))
```

```{r plotting}
# put cosines into same column with order ("gram") in a different column
tidy_corr_data <- distances %>%
  pivot_longer(c(unigram_cosine, trigram_cosine), names_to = "gram", values_to = "angle") %>%
  mutate(gram = if_else(gram == "unigram_cosine", "unigram", "trigram"),
         gram = factor(gram, levels = c("unigram", "trigram"))) %>%
  mutate(feature = scale(as.double(feature)) %>% as.numeric()) %>%
  group_by(gram) %>%
  mutate(angle = 1 - (acos(angle) / pi),
         angle = exp(angle),
         angle = scale(angle))

# how are unigram and trigram angle measures correlated? r^2 = .2171
tidy_corr_data %>% 
  ungroup() %>% 
  pivot_wider(names_from = gram, values_from = angle) %>%
  summarise(cor_gram = cor(unigram, trigram),
            cor_uni = cor(unigram, feature),
            cor_tri = cor(trigram, feature))

# 
# %>% 
#   ggplot(aes(x = unigram, y =trigram)) + 
#     geom_jitter(alpha = .05) + 
#     geom_smooth(method = "lm", se=F)

# unigram and trigram angle measures predict number of WALS features in common (small r^2)
ggplot(tidy_corr_data, aes(x = ldn, y = angle, fill = gram,
                           color = gram, group = gram,
                           label = gram)) + 
  geom_smooth(method = "lm") +
  #geom_jitter(alpha = .05) +
  geom_dl(method = "smart.grid") + 
  theme(legend.position = "none") + 
  xlab("Mean normalized Levenshtein Distance") + 
  ylab("Information curve distance")

model <- lmer(angle ~ feature * gram + (1|language1) + (1|language2),
     data = tidy_corr_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect)

View(model)
  
```

```{r compute trigram cosine distances}
trigram_slopes <- read_csv(here("Data/Wikipedia/new_relative_slopes.csv"))
ldn_unigram_wals <- read_feather(here("Data/Wikipedia/ldn_unigrams_wals_pairs.feather"))

# compute trigram cosines: pairs of languages map to their cosine distances
trigram_cosines <- trigram_slopes %>%
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_tibble(rownames = "language1") %>%
  pivot_longer(-language1, names_to = "language2", values_to = "trigram_cosine")

# add trigram cosines to stored language distance measures
ldn_unigram_wals %>% 
  left_join(trigram_cosines) %>%
  write_feather(here("Data/Wikipedia/all_spaces.feather"))
```

```{r feature similarity within feature category}
# pass in a dataframe of wals category values (without language)
# get back the average entropy of the dataframe
category_entropy <- function(df) {
  df %>%
    lapply(table) %>%
    lapply(entropy.empirical) %>%
    unlist() %>%
    mean()
} # range is [0, ~5]

features <- read_csv(here("Data/Wikipedia/wals_features.csv")) %>%
  pivot_longer(-language, names_to = "feature", values_to = "value") %>%
  left_join(read_csv(here("Data/feature_types.csv")), by = "feature")

category_entropy <- features %>%
  group_by(type) %>%
  nest() %>%
  mutate(entropy = map(data, category_entropy)) %>%
  select(-data) %>%
  unnest(cols = c(entropy))
```


```{r}
compute_sim <- function(language1, language2, feature_type) {
  
  features %>%
    filter(language == language1, type == feature_type) %>%
    rename(value1 = value) %>% 
    left_join(features %>% filter(language == language2,type == feature_type) %>%
                select(feature, value), 
              by = "feature") %>%
    mutate(same = value == value1) %>%
    ungroup() %>%
    group_by(language, type) %>%
    summarise(feature = sum(same)) %>%
    mutate(language2 = language2) %>%
    rename(language1 = language)
}

feature_sim_combos <- expand_grid(language1 = unique(features$language),
            language2 = unique(features$language),
            type = unique(features$type)) %>%
  filter(language1 != language2)

## this takes a long time to run
feature_sims <- map_dfr(1:nrow(feature_sims), 
                        ~compute_sim(feature_sim_combos %>% 
                                       slice(.x) %>% pull(language1),
                                     feature_sim_combos %>% slice(.x) %>% 
                                       pull(language2),
                                     feature_sim_combos %>% slice(.x) %>% 
                                       pull(type)))

tidy_features <- feature_sims %>%
  group_by(type) %>%
  mutate(feature = scale(feature)) 

category_data <- tidy_corr_data %>%
  select(-feature) %>%
  distinct() %>%
  left_join(tidy_features, by = c("language1", "language2")) %>%
  left_join(category_entropy, by = c("type")) #%>%
  #bind_rows(mutate(tidy_corr_data, type = "all"))


category_model <- lmer(angle ~ 0 + 
                         feature : type : gram + 
                         (1|language1) + (1|language2), 
     data = category_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect) 

category_model %>%
  mutate_at(vars(-term), round, digits = 2) %>%
  mutate(order = str_detect(term, "trigram")) %>%
  arrange(order, term) %>%
  select(-order) %>%
  View()


````
