---
title: "Wikipedia information curve analysis"
author: "Josef Klafka and Daniel Yurovsky"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
require(tidyverse)
require(lsa)
require(broom)
library(broom.mixed)
require(janitor)
require(feather)
require(here)
require(lingtypology)
require(lme4)
require(directlabels)

knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
distances <- read_feather(here("Data/Wikipedia/all_spaces.feather"))
```

```{r plotting}
# put cosines into same column with order ("gram") in a different column
tidy_corr_data <- distances %>%
  pivot_longer(c(unigram_cosine, trigram_cosine), names_to = "gram", values_to = "angle") %>%
  mutate(gram = if_else(gram == "unigram_cosine", "unigram", "trigram"),
         gram = factor(gram, levels = c("unigram", "trigram"))) %>%
  mutate(feature = scale(feature)) %>%
  group_by(gram) %>%
  mutate(angle = 1 - (acos(angle) / pi),
         angle = exp(angle),
         angle = scale(angle))

# how are unigram and trigram angle measures correlated? r^2 = .2171
tidy_corr_data %>% 
  ungroup() %>% 
  pivot_wider(names_from = gram, values_from = angle) %>% 
  ggplot(aes(x = unigram, y =trigram)) + 
    geom_jitter(alpha = .05) + 
    geom_smooth(method = "lm", se=F)

# unigram and trigram angle measures predict number of WALS features in common (small r^2)
ggplot(tidy_corr_data, aes(x = ldn, y = angle, fill = gram,
                           color = gram, group = gram,
                           label = gram)) + 
  geom_smooth(method = "lm") +
  #geom_jitter(alpha = .05) +
  geom_dl(method = "smart.grid") + 
  theme(legend.position = "none") + 
  xlab("Mean normalized Levenshtein Distance") + 
  ylab("Information curve distance")

model <- lmer(angle ~ feature * gram + (1|language1) + (1|language2),
     data = tidy_corr_data) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect)

View(model)
  
```

```{r compute trigram cosine distances}
trigram_slopes <- read_csv(here("Data/Wikipedia/new_relative_slopes.csv"))
ldn_unigram_wals <- read_feather(here("Data/Wikipedia/ldn_unigrams_wals_pairs.feather"))

# compute trigram cosines: pairs of languages map to their cosine distances
trigram_cosines <- trigram_slopes %>%
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_tibble(rownames = "language1") %>%
  pivot_longer(-language1, names_to = "language2", values_to = "trigram_cosine")

# add trigram cosines to stored language distance measures
ldn_unigram_wals %>% 
  left_join(trigram_cosines) %>%
  write_feather(here("Data/Wikipedia/all_spaces.feather"))
```

```{r feature similarity within feature category}
# pass in a dataframe of wals category values (without language)
# get back the average entropy of the dataframe
category_entropy <- function(df) {
  df %>%
    lapply(table) %>%
    lapply(entropy.empirical) %>%
    unlist() %>%
    mean()
} # range is [0, ~5]

features <- read_csv(here("Data/Wikipedia/wals_features.csv")) %>%
  clean_names()

#phonology  
features %>%
  select(x1a:x29a) %>% 
  category_entropy()

# nom_categories 
features %>%
  select(x30a:x57a) %>% 
  category_entropy()

# nom_syntax 
features %>%
  select(x58a:x64a) %>% 
  category_entropy()

# verb_categories 
features %>%
  select(x65a:x80a) %>% 
  category_entropy()

# word_order
features %>%
  select(x81a:x97a) %>% 
  category_entropy()

# clauses 
features %>%
  select(x98a:x120a) %>% 
  category_entropy()
```

